{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prerequisite-copper",
   "metadata": {
    "id": "modular-forth"
   },
   "source": [
    "# Implementación de DenseNet para las dos vistas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-warner",
   "metadata": {
    "id": "younger-sentence"
   },
   "source": [
    "Ajustamos el notebook según estemos trabajando en local o en un entorno de Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "outdoor-peripheral",
   "metadata": {
    "executionInfo": {
     "elapsed": 975,
     "status": "ok",
     "timestamp": 1621188710580,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "flying-consciousness"
   },
   "outputs": [],
   "source": [
    "google_colab = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-tonight",
   "metadata": {},
   "source": [
    "Importamos todas las librerías necesarias para la implementación del entrenamiento de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stone-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "\n",
    "if google_colab:\n",
    "    from google.colab import drive\n",
    "    !pip install pickle5\n",
    "    import pickle5 as pickle\n",
    "    drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-defense",
   "metadata": {
    "id": "engaging-persian"
   },
   "source": [
    "##  Carga del dataset y preparación de los dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-mountain",
   "metadata": {
    "id": "beautiful-monthly"
   },
   "source": [
    "Definimos una función auxiliar para ayudar con el preprocesamiento de los datos (ajuste de entrada para la DenseNet en el caso de las imágenes y conversión a one-hot encoding para las etiquetas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "essential-wayne",
   "metadata": {
    "executionInfo": {
     "elapsed": 22216,
     "status": "ok",
     "timestamp": 1621188731832,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "traditional-islam"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X_cc, X_mlo, Y):\n",
    "    \"\"\"\n",
    "    Pre-processes the data for the model\n",
    "        - X_cc is a numpy.ndarray of shape (m, 1024, 1024, 3) containing\n",
    "         the CC view mammography, where m is the number of data points\n",
    "        - X_mlo is a numpy.ndarray of shape (m, 1024, 1024, 3) containing\n",
    "         the MLO view mammography, where m is the number of data points\n",
    "        - Y is a numpy.ndarray of shape (m,) containing\n",
    "         the Bi-Rads labels for X\n",
    "    Returns:\n",
    "        - X_cc_p is a numpy.ndarray containing the preprocessed X_cc\n",
    "        - X_mlo_p is a numpy.ndarray containing the preprocessed X_mlo\n",
    "        - Y_p is a numpy.ndarray containing the preprocessed Y\n",
    "    \"\"\"\n",
    "    X_cc_p = K.applications.densenet.preprocess_input(X_cc)\n",
    "    X_mlo_p = K.applications.densenet.preprocess_input(X_mlo)\n",
    "\n",
    "    Y_p = K.utils.to_categorical(Y, 3)\n",
    "\n",
    "    return X_cc_p, X_mlo_p, Y_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-behalf",
   "metadata": {
    "id": "mobile-monroe"
   },
   "source": [
    "Cargamos los ficheros de entrada, tanto el de entrenamiento-test como el de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ideal-shield",
   "metadata": {
    "executionInfo": {
     "elapsed": 39717,
     "status": "ok",
     "timestamp": 1621188749334,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "frequent-homework"
   },
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "    with open('/content/gdrive/MyDrive/Colab Notebooks/df_INbreast_train.pkl', 'rb') as pickle_file:\n",
    "        df_INbreast_train = pickle.load(pickle_file)\n",
    "    with open('/content/gdrive/MyDrive/Colab Notebooks/df_INbreast_val.pkl', 'rb') as pickle_file:\n",
    "        df_INbreast_val = pickle.load(pickle_file)\n",
    "else:\n",
    "    df_INbreast_train = pd.read_pickle('./df_INbreast_train.pkl')\n",
    "    df_INbreast_val = pd.read_pickle('./df_INbreast_val.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-saint",
   "metadata": {
    "id": "contained-franchise"
   },
   "source": [
    "Cargamos los datos, convertimos las etiquetas a enteros y liberamos espacio de los ficheros que contenían el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "electrical-showcase",
   "metadata": {
    "executionInfo": {
     "elapsed": 41835,
     "status": "ok",
     "timestamp": 1621188751455,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "fatty-forwarding"
   },
   "outputs": [],
   "source": [
    "dict_valores = {'benigno': 0, 'seguimiento': 1, 'maligno': 2}\n",
    "Y_traintest = np.array(df_INbreast_train['Bi-Rads'].map(dict_valores).tolist())\n",
    "X_traintest_cc = np.array(df_INbreast_train['CC Image'].tolist())\n",
    "X_traintest_mlo = np.array(df_INbreast_train['MLO Image'].tolist())\n",
    "Y_val = np.array(df_INbreast_val['Bi-Rads'].map(dict_valores).tolist())\n",
    "X_val_cc = np.array(df_INbreast_val['CC Image'].tolist())\n",
    "X_val_mlo = np.array(df_INbreast_val['MLO Image'].tolist())\n",
    "X_val_cc, X_val_mlo, Y_val = preprocess_data(X_val_cc, X_val_mlo, Y_val)\n",
    "del df_INbreast_train\n",
    "del df_INbreast_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-spending",
   "metadata": {
    "id": "W4IVXDMAOplr"
   },
   "source": [
    "## Definición de la arquitectura de red neuronal de dos ramas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-chinese",
   "metadata": {
    "id": "5moG-NYZcICz"
   },
   "source": [
    "Cargamos los modelos individuales de cada rama y definimos el nuevo modelo a partir de ellos, junto con el inicializador de los pesos de la capa conectada y el optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wound-folks",
   "metadata": {
    "executionInfo": {
     "elapsed": 95117,
     "status": "ok",
     "timestamp": 1621188804739,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "8Yka6jqyOtSx"
   },
   "outputs": [],
   "source": [
    "model_cc = K.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/model_CC')\n",
    "model_mlo = K.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/model_MLO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "genuine-nursery",
   "metadata": {
    "executionInfo": {
     "elapsed": 95116,
     "status": "ok",
     "timestamp": 1621188804740,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "7UvQIbcNPxqL"
   },
   "outputs": [],
   "source": [
    "def DenseNet_2Ramas(model_cc, model_mlo, rand_seed = 2021, learning_rate = 0.0001, momentum = 0.9):\n",
    "    \"\"\"\n",
    "    Define the DenseNet architecture and compile the model\n",
    "        - model_cc is the pretrained CC-view DenseNet model\n",
    "        - model_mlo is the pretrained MLO-view DenseNet model\n",
    "        - rand_seed is a random seed number used during the fc layer initialization\n",
    "        - learning_rate is the learning rate used during the training\n",
    "        - momentum is the parameters that defines the momentun for the SGD optimizer\n",
    "    Returns:\n",
    "        - model_2ramas is the output compiled DenseNet 2-branch model\n",
    "    \"\"\"\n",
    "    # Define the model architecture\n",
    "    model_cc = K.Sequential(model_cc.layers[:-1])\n",
    "    model_mlo = K.Sequential(model_mlo.layers[:-1])\n",
    "\n",
    "    model_cc.layers[-1]._name = model_cc.layers[-1].name + '_cc'\n",
    "    model_mlo.layers[-1]._name = model_mlo.layers[-1].name + '_mlo'\n",
    "\n",
    "    model_cc.trainable = False\n",
    "    model_mlo.trainable = False\n",
    "\n",
    "    combined = K.layers.Concatenate()([model_cc.output, model_mlo.output])\n",
    "    \n",
    "    initializer = K.initializers.he_normal(seed = rand_seed)\n",
    "    \n",
    "    fc_layer = K.layers.Dense(units = 3,\n",
    "                              activation = 'softmax',\n",
    "                              kernel_initializer = initializer\n",
    "                              )(combined)\n",
    "\n",
    "    model_2ramas = K.models.Model(inputs = [model_cc.input, model_mlo.input], outputs = fc_layer)\n",
    "\n",
    "    # Compile the model\n",
    "    opt = K.optimizers.SGD(learning_rate = learning_rate, momentum = momentum)\n",
    "    \n",
    "    model_2ramas.compile(loss = 'categorical_crossentropy',\n",
    "                         optimizer = opt,\n",
    "                         metrics = ['accuracy'])\n",
    "    \n",
    "    return model_2ramas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-count",
   "metadata": {
    "id": "LNizgVEZ5ENX"
   },
   "source": [
    "Mostramos por pantalla la arquitectura de la red definida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "visible-benjamin",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96767,
     "status": "ok",
     "timestamp": 1621188806394,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "M_Sbv12gckjn",
    "outputId": "1f397139-8bec-41b2-ecac-e96c8c3fd8f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resize_CC (Lambda)              (None, 256, 256, 3)  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "resize_MLO (Lambda)             (None, 256, 256, 3)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "densenet121_cc (Functional)     (None, 1024)         7037504     resize_CC[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "densenet121_mlo (Functional)    (None, 1024)         7037504     resize_MLO[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2048)         0           densenet121_cc[1][0]             \n",
      "                                                                 densenet121_mlo[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            6147        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 14,081,155\n",
      "Trainable params: 6,147\n",
      "Non-trainable params: 14,075,008\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DenseNet_2Ramas(model_cc, model_mlo).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-pocket",
   "metadata": {
    "id": "daily-secretariat"
   },
   "source": [
    "## Entrenamiento de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-explorer",
   "metadata": {
    "id": "AVvOsz7p5ptL"
   },
   "source": [
    "Definimos una función auxiliar que particiona el conjunto de entrenamiento/test en los dos subconjuntos correspondientes (entrenamiento y test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "delayed-result",
   "metadata": {
    "executionInfo": {
     "elapsed": 96766,
     "status": "ok",
     "timestamp": 1621188806394,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "MfoWhBMq5plj"
   },
   "outputs": [],
   "source": [
    "def part_traintest(X_traintest_cc, X_traintest_mlo, Y_traintest, rand_seed = 2021, frac_test = .2/.8):\n",
    "    \"\"\"\n",
    "    Function that makes a partition for training and testing from the original dataset\n",
    "        - X_traintest_cc is the array of CC-view images from the original dataset\n",
    "        - X_traintest_mlo is the array of MLO-view images from the original dataset\n",
    "        - Y_traintest is the array of labels from the original dataset\n",
    "        - rand_seed is a random seed number used during the sampling\n",
    "        - frac_test is the fraction of cases used in the test subset\n",
    "    Returns:\n",
    "        - X_train_cc is the train array of CC-images\n",
    "        - X_train_mlo is the train array of MLO-images\n",
    "        - Y_train is the train array of labels\n",
    "        - X_test_cc is the test array of CC-images\n",
    "        - X_test_mlo is the test array of MLO-images\n",
    "        - Y_test is the test array of labels\n",
    "    \"\"\"\n",
    "    np.random.seed(rand_seed)\n",
    "    index_test = np.array([], dtype = 'int64')\n",
    "    for i in np.unique(Y_traintest):\n",
    "        index_test = np.append(index_test, \n",
    "                               np.random.choice(list(np.where(Y_traintest == i)[0]), size = int(np.where(Y_traintest == i)[0].shape[0]*frac_test), replace = False))\n",
    "    X_train_cc = np.delete(X_traintest_cc, index_test, axis = 0)\n",
    "    X_train_mlo = np.delete(X_traintest_mlo, index_test, axis = 0)\n",
    "    Y_train = np.delete(Y_traintest, index_test)\n",
    "    X_test_cc = np.take(X_traintest_cc, index_test, axis = 0)\n",
    "    X_test_mlo = np.take(X_traintest_mlo, index_test, axis = 0)\n",
    "    Y_test = np.take(Y_traintest, index_test)\n",
    "    X_train_cc, X_train_mlo, Y_train = preprocess_data(X_train_cc, X_train_mlo, Y_train)\n",
    "    X_test_cc, X_test_mlo, Y_test = preprocess_data(X_test_cc, X_test_mlo, Y_test)\n",
    "\n",
    "    return X_train_cc, X_train_mlo, Y_train, X_test_cc, X_test_mlo, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-hormone",
   "metadata": {
    "id": "korean-press"
   },
   "source": [
    "Definimos los parámetros básicos para el proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "digital-factor",
   "metadata": {
    "executionInfo": {
     "elapsed": 96766,
     "status": "ok",
     "timestamp": 1621188806395,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "incident-sender"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "no_epochs = 300\n",
    "rand_seed = 2021\n",
    "learning_rate = 0.001\n",
    "momentum = 0.8\n",
    "n_folds = 6\n",
    "frac_test = .2/.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-rapid",
   "metadata": {
    "id": "0j95AaVx5bCa"
   },
   "source": [
    "Dado el desbalance que sufren las categorías de la muestra de entrenamiento, forzamos el balanceo calculando las proporciones respecto a la clase más representada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "closing-blackberry",
   "metadata": {
    "executionInfo": {
     "elapsed": 96765,
     "status": "ok",
     "timestamp": 1621188806396,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "pb5VnDmd5bCe"
   },
   "outputs": [],
   "source": [
    "label, counts = np.unique(Y_traintest, return_counts = True)\n",
    "class_weight = {}\n",
    "for lab, con in zip(label, counts):\n",
    "  class_weight.update({lab: round(max(counts)/con, 2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-amateur",
   "metadata": {
    "id": "GKOHQP5Q5e47"
   },
   "source": [
    "Definimos un callback para el entrenamiento de la red, de tal manera que nos aseguramos que el entrenamiento disminuye el learning rate cuando la pérdida sobre el conjutno de test ya no disminuye y detenemos el entrenamiento cuando dich pérdida tampoco disminuye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "functional-space",
   "metadata": {
    "executionInfo": {
     "elapsed": 96764,
     "status": "ok",
     "timestamp": 1621188806397,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "s_zS0yCB5gZ0"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience = 20, restore_best_weights = True)\n",
    "reduce_lr = ReduceLROnPlateau(factor = 0.5, patience = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-motel",
   "metadata": {
    "id": "yVvCVm45MlNh"
   },
   "source": [
    "Iteramos la definición y el entrenamiento de la red para no sesgar los resultados según el conjunto de entrenamiento y de test escogido en cada caso. Almacenamos el output de cada iteración para poder representarlos más adelante, evaluando cada red obtenida mediante el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "injured-arrival",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1433256,
     "status": "ok",
     "timestamp": 1621190142892,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "studied-billy",
    "outputId": "d8f46312-b88f-4cc0-a97e-cbdb91bbf7d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1/6 ...\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/300\n",
      "10/10 [==============================] - 53s 945ms/step - loss: 1.2024 - accuracy: 0.6972 - val_loss: 0.5214 - val_accuracy: 0.8654\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 1.0187 - accuracy: 0.6870 - val_loss: 0.6823 - val_accuracy: 0.6923\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.8028 - accuracy: 0.7766 - val_loss: 0.4672 - val_accuracy: 0.9038\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 2s 207ms/step - loss: 0.4718 - accuracy: 0.9133 - val_loss: 0.3947 - val_accuracy: 0.9231\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.4233 - accuracy: 0.9345 - val_loss: 0.3762 - val_accuracy: 0.9038\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.3426 - accuracy: 0.9416 - val_loss: 0.3919 - val_accuracy: 0.9038\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.3409 - accuracy: 0.9578 - val_loss: 0.3752 - val_accuracy: 0.8846\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.2665 - accuracy: 0.9747 - val_loss: 0.3777 - val_accuracy: 0.9038\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.2682 - accuracy: 0.9608 - val_loss: 0.3788 - val_accuracy: 0.8846\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.2446 - accuracy: 0.9854 - val_loss: 0.3645 - val_accuracy: 0.8846\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.2608 - accuracy: 0.9520 - val_loss: 0.3566 - val_accuracy: 0.9231\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.2367 - accuracy: 0.9670 - val_loss: 0.3600 - val_accuracy: 0.9231\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.2073 - accuracy: 0.9957 - val_loss: 0.3595 - val_accuracy: 0.9231\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.1791 - accuracy: 0.9933 - val_loss: 0.3628 - val_accuracy: 0.9038\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.2097 - accuracy: 0.9828 - val_loss: 0.3517 - val_accuracy: 0.8846\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1892 - accuracy: 0.9907 - val_loss: 0.3713 - val_accuracy: 0.8654\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1665 - accuracy: 0.9934 - val_loss: 0.4003 - val_accuracy: 0.8077\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1771 - accuracy: 0.9767 - val_loss: 0.3703 - val_accuracy: 0.8654\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1842 - accuracy: 1.0000 - val_loss: 0.3434 - val_accuracy: 0.9038\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1372 - accuracy: 0.9988 - val_loss: 0.3926 - val_accuracy: 0.8077\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.1706 - accuracy: 0.9700 - val_loss: 0.3863 - val_accuracy: 0.8269\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1557 - accuracy: 0.9932 - val_loss: 0.3406 - val_accuracy: 0.9038\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1441 - accuracy: 0.9885 - val_loss: 0.3617 - val_accuracy: 0.8846\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1561 - accuracy: 0.9988 - val_loss: 0.3498 - val_accuracy: 0.8846\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1278 - accuracy: 0.9982 - val_loss: 0.3419 - val_accuracy: 0.9231\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1198 - accuracy: 1.0000 - val_loss: 0.3515 - val_accuracy: 0.8846\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1126 - accuracy: 1.0000 - val_loss: 0.3427 - val_accuracy: 0.9038\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1150 - accuracy: 1.0000 - val_loss: 0.3498 - val_accuracy: 0.8846\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1168 - accuracy: 1.0000 - val_loss: 0.3498 - val_accuracy: 0.8846\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1070 - accuracy: 1.0000 - val_loss: 0.3437 - val_accuracy: 0.9038\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1162 - accuracy: 1.0000 - val_loss: 0.3453 - val_accuracy: 0.8846\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1120 - accuracy: 1.0000 - val_loss: 0.3447 - val_accuracy: 0.8846\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 2s 206ms/step - loss: 0.1027 - accuracy: 1.0000 - val_loss: 0.3484 - val_accuracy: 0.8846\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1080 - accuracy: 1.0000 - val_loss: 0.3493 - val_accuracy: 0.8846\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1154 - accuracy: 1.0000 - val_loss: 0.3460 - val_accuracy: 0.8846\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1092 - accuracy: 1.0000 - val_loss: 0.3451 - val_accuracy: 0.8846\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1003 - accuracy: 1.0000 - val_loss: 0.3464 - val_accuracy: 0.8846\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1226 - accuracy: 1.0000 - val_loss: 0.3481 - val_accuracy: 0.8846\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1067 - accuracy: 1.0000 - val_loss: 0.3449 - val_accuracy: 0.8846\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1027 - accuracy: 1.0000 - val_loss: 0.3463 - val_accuracy: 0.8846\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1102 - accuracy: 1.0000 - val_loss: 0.3460 - val_accuracy: 0.8846\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1038 - accuracy: 1.0000 - val_loss: 0.3481 - val_accuracy: 0.8846\n",
      "------------------------------------------------------------------------\n",
      "Score for fold 1: loss of 0.6; accuracy of 71.15%\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2/6 ...\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/300\n",
      "10/10 [==============================] - 16s 567ms/step - loss: 0.9365 - accuracy: 0.7558 - val_loss: 0.7252 - val_accuracy: 0.7500\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.9286 - accuracy: 0.7485 - val_loss: 0.5584 - val_accuracy: 0.8462\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.6840 - accuracy: 0.8902 - val_loss: 0.6406 - val_accuracy: 0.7500\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 2s 207ms/step - loss: 0.5846 - accuracy: 0.8944 - val_loss: 0.4946 - val_accuracy: 0.8846\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.5425 - accuracy: 0.9076 - val_loss: 0.3340 - val_accuracy: 0.9615\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.4089 - accuracy: 0.9263 - val_loss: 0.2993 - val_accuracy: 0.9231\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.3665 - accuracy: 0.9305 - val_loss: 0.3196 - val_accuracy: 0.9231\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.3657 - accuracy: 0.9393 - val_loss: 0.3064 - val_accuracy: 0.9615\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.3724 - accuracy: 0.9354 - val_loss: 0.2599 - val_accuracy: 0.9231\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.3186 - accuracy: 0.9612 - val_loss: 0.2967 - val_accuracy: 0.9423\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.3001 - accuracy: 0.9562 - val_loss: 0.3917 - val_accuracy: 0.8462\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.2927 - accuracy: 0.9332 - val_loss: 0.2886 - val_accuracy: 0.9423\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.2337 - accuracy: 0.9732 - val_loss: 0.2598 - val_accuracy: 0.9423\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.2750 - accuracy: 0.9638 - val_loss: 0.2480 - val_accuracy: 0.9423\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.2158 - accuracy: 0.9860 - val_loss: 0.2704 - val_accuracy: 0.9231\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.2343 - accuracy: 0.9773 - val_loss: 0.2486 - val_accuracy: 0.9231\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.2174 - accuracy: 0.9727 - val_loss: 0.2537 - val_accuracy: 0.9231\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.2169 - accuracy: 0.9692 - val_loss: 0.2753 - val_accuracy: 0.8846\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.2060 - accuracy: 0.9878 - val_loss: 0.2686 - val_accuracy: 0.9231\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.2110 - accuracy: 0.9717 - val_loss: 0.2473 - val_accuracy: 0.9423\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1890 - accuracy: 0.9786 - val_loss: 0.2399 - val_accuracy: 0.9231\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1823 - accuracy: 0.9744 - val_loss: 0.2405 - val_accuracy: 0.9231\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1670 - accuracy: 0.9834 - val_loss: 0.2396 - val_accuracy: 0.9231\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1761 - accuracy: 0.9831 - val_loss: 0.2386 - val_accuracy: 0.9423\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1869 - accuracy: 0.9752 - val_loss: 0.2421 - val_accuracy: 0.9423\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1706 - accuracy: 0.9845 - val_loss: 0.2374 - val_accuracy: 0.9231\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1655 - accuracy: 0.9928 - val_loss: 0.2375 - val_accuracy: 0.9231\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1515 - accuracy: 0.9880 - val_loss: 0.2376 - val_accuracy: 0.9231\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 2s 207ms/step - loss: 0.1513 - accuracy: 0.9915 - val_loss: 0.2367 - val_accuracy: 0.9423\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1721 - accuracy: 0.9878 - val_loss: 0.2356 - val_accuracy: 0.9231\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1674 - accuracy: 0.9767 - val_loss: 0.2349 - val_accuracy: 0.9231\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1516 - accuracy: 0.9767 - val_loss: 0.2361 - val_accuracy: 0.9423\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1682 - accuracy: 0.9717 - val_loss: 0.2353 - val_accuracy: 0.9231\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1590 - accuracy: 0.9828 - val_loss: 0.2369 - val_accuracy: 0.9423\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1517 - accuracy: 0.9826 - val_loss: 0.2338 - val_accuracy: 0.9423\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1502 - accuracy: 0.9955 - val_loss: 0.2323 - val_accuracy: 0.9231\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1745 - accuracy: 0.9982 - val_loss: 0.2324 - val_accuracy: 0.9231\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1540 - accuracy: 0.9828 - val_loss: 0.2327 - val_accuracy: 0.9231\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1280 - accuracy: 1.0000 - val_loss: 0.2306 - val_accuracy: 0.9231\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1425 - accuracy: 1.0000 - val_loss: 0.2323 - val_accuracy: 0.9231\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1353 - accuracy: 0.9975 - val_loss: 0.2310 - val_accuracy: 0.9231\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1322 - accuracy: 1.0000 - val_loss: 0.2311 - val_accuracy: 0.9231\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1318 - accuracy: 1.0000 - val_loss: 0.2305 - val_accuracy: 0.9423\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1244 - accuracy: 0.9933 - val_loss: 0.2297 - val_accuracy: 0.9423\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1444 - accuracy: 1.0000 - val_loss: 0.2296 - val_accuracy: 0.9231\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1056 - accuracy: 0.9988 - val_loss: 0.2298 - val_accuracy: 0.9231\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1319 - accuracy: 0.9988 - val_loss: 0.2292 - val_accuracy: 0.9231\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1241 - accuracy: 0.9988 - val_loss: 0.2288 - val_accuracy: 0.9231\n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1300 - accuracy: 0.9949 - val_loss: 0.2302 - val_accuracy: 0.9231\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1216 - accuracy: 1.0000 - val_loss: 0.2291 - val_accuracy: 0.9231\n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1060 - accuracy: 0.9967 - val_loss: 0.2286 - val_accuracy: 0.9231\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1178 - accuracy: 0.9957 - val_loss: 0.2278 - val_accuracy: 0.9231\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1145 - accuracy: 1.0000 - val_loss: 0.2294 - val_accuracy: 0.9231\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1234 - accuracy: 1.0000 - val_loss: 0.2268 - val_accuracy: 0.9231\n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1178 - accuracy: 1.0000 - val_loss: 0.2280 - val_accuracy: 0.9231\n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1152 - accuracy: 1.0000 - val_loss: 0.2272 - val_accuracy: 0.9231\n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1171 - accuracy: 1.0000 - val_loss: 0.2259 - val_accuracy: 0.9231\n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1146 - accuracy: 1.0000 - val_loss: 0.2276 - val_accuracy: 0.9231\n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1182 - accuracy: 1.0000 - val_loss: 0.2259 - val_accuracy: 0.9231\n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1264 - accuracy: 1.0000 - val_loss: 0.2269 - val_accuracy: 0.9231\n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1272 - accuracy: 1.0000 - val_loss: 0.2269 - val_accuracy: 0.9231\n",
      "Epoch 62/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1214 - accuracy: 1.0000 - val_loss: 0.2260 - val_accuracy: 0.9231\n",
      "Epoch 63/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1010 - accuracy: 1.0000 - val_loss: 0.2255 - val_accuracy: 0.9231\n",
      "Epoch 64/300\n",
      "10/10 [==============================] - 2s 205ms/step - loss: 0.0945 - accuracy: 1.0000 - val_loss: 0.2254 - val_accuracy: 0.9231\n",
      "Epoch 65/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0998 - accuracy: 1.0000 - val_loss: 0.2253 - val_accuracy: 0.9231\n",
      "Epoch 66/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0915 - accuracy: 1.0000 - val_loss: 0.2252 - val_accuracy: 0.9231\n",
      "Epoch 67/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1055 - accuracy: 1.0000 - val_loss: 0.2260 - val_accuracy: 0.9231\n",
      "Epoch 68/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1163 - accuracy: 1.0000 - val_loss: 0.2258 - val_accuracy: 0.9231\n",
      "Epoch 69/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0911 - accuracy: 1.0000 - val_loss: 0.2254 - val_accuracy: 0.9231\n",
      "Epoch 70/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1056 - accuracy: 1.0000 - val_loss: 0.2256 - val_accuracy: 0.9231\n",
      "Epoch 71/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.0985 - accuracy: 1.0000 - val_loss: 0.2256 - val_accuracy: 0.9038\n",
      "Epoch 72/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1122 - accuracy: 1.0000 - val_loss: 0.2257 - val_accuracy: 0.9038\n",
      "Epoch 73/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0972 - accuracy: 1.0000 - val_loss: 0.2252 - val_accuracy: 0.9038\n",
      "Epoch 74/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0950 - accuracy: 1.0000 - val_loss: 0.2254 - val_accuracy: 0.9231\n",
      "Epoch 75/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1089 - accuracy: 1.0000 - val_loss: 0.2252 - val_accuracy: 0.9231\n",
      "Epoch 76/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1107 - accuracy: 1.0000 - val_loss: 0.2252 - val_accuracy: 0.9231\n",
      "Epoch 77/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1014 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 78/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0915 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 79/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0934 - accuracy: 1.0000 - val_loss: 0.2253 - val_accuracy: 0.9231\n",
      "Epoch 80/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0888 - accuracy: 1.0000 - val_loss: 0.2253 - val_accuracy: 0.9231\n",
      "Epoch 81/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.0978 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 82/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1023 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 83/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1075 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 84/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0979 - accuracy: 1.0000 - val_loss: 0.2250 - val_accuracy: 0.9231\n",
      "Epoch 85/300\n",
      "10/10 [==============================] - 2s 206ms/step - loss: 0.0964 - accuracy: 1.0000 - val_loss: 0.2250 - val_accuracy: 0.9231\n",
      "Epoch 86/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1067 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 87/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1019 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 88/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.0988 - accuracy: 1.0000 - val_loss: 0.2250 - val_accuracy: 0.9231\n",
      "Epoch 89/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.0933 - accuracy: 1.0000 - val_loss: 0.2250 - val_accuracy: 0.9231\n",
      "Epoch 90/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1046 - accuracy: 1.0000 - val_loss: 0.2250 - val_accuracy: 0.9231\n",
      "Epoch 91/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0966 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 92/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0952 - accuracy: 1.0000 - val_loss: 0.2250 - val_accuracy: 0.9231\n",
      "Epoch 93/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1045 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 94/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.0981 - accuracy: 1.0000 - val_loss: 0.2250 - val_accuracy: 0.9231\n",
      "Epoch 95/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0975 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 96/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1070 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 97/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.0922 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 98/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0834 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 99/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1040 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 100/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0959 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 101/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1010 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 102/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1044 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 103/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1010 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 104/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0943 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 105/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0903 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 106/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0939 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 107/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1057 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 108/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1083 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "Epoch 109/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0929 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9231\n",
      "------------------------------------------------------------------------\n",
      "Score for fold 2: loss of 0.64; accuracy of 75.0%\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3/6 ...\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/300\n",
      "10/10 [==============================] - 15s 553ms/step - loss: 1.0040 - accuracy: 0.7323 - val_loss: 0.6267 - val_accuracy: 0.7308\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.9682 - accuracy: 0.7368 - val_loss: 0.4852 - val_accuracy: 0.8462\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.7149 - accuracy: 0.8047 - val_loss: 0.7569 - val_accuracy: 0.6731\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.6958 - accuracy: 0.7841 - val_loss: 0.5119 - val_accuracy: 0.8462\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.4608 - accuracy: 0.9329 - val_loss: 0.3873 - val_accuracy: 0.8269\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.3879 - accuracy: 0.9090 - val_loss: 0.3850 - val_accuracy: 0.8462\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.3255 - accuracy: 0.9507 - val_loss: 0.3670 - val_accuracy: 0.8269\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.3413 - accuracy: 0.9339 - val_loss: 0.4010 - val_accuracy: 0.8269\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.3109 - accuracy: 0.9163 - val_loss: 0.3411 - val_accuracy: 0.8462\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.2854 - accuracy: 0.9745 - val_loss: 0.3605 - val_accuracy: 0.8654\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.2429 - accuracy: 0.9758 - val_loss: 0.3890 - val_accuracy: 0.8846\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.2479 - accuracy: 0.9586 - val_loss: 0.3338 - val_accuracy: 0.9231\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.2163 - accuracy: 0.9831 - val_loss: 0.3402 - val_accuracy: 0.8846\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.2323 - accuracy: 0.9946 - val_loss: 0.3343 - val_accuracy: 0.9038\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.2124 - accuracy: 0.9873 - val_loss: 0.3396 - val_accuracy: 0.8846\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1939 - accuracy: 0.9933 - val_loss: 0.3204 - val_accuracy: 0.8846\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1908 - accuracy: 0.9904 - val_loss: 0.3398 - val_accuracy: 0.9038\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1840 - accuracy: 1.0000 - val_loss: 0.3249 - val_accuracy: 0.8846\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1856 - accuracy: 0.9795 - val_loss: 0.3526 - val_accuracy: 0.8846\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1690 - accuracy: 0.9848 - val_loss: 0.3216 - val_accuracy: 0.8846\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1642 - accuracy: 0.9845 - val_loss: 0.3174 - val_accuracy: 0.8846\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1755 - accuracy: 0.9853 - val_loss: 0.3147 - val_accuracy: 0.9038\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1667 - accuracy: 0.9913 - val_loss: 0.3151 - val_accuracy: 0.8654\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1552 - accuracy: 0.9967 - val_loss: 0.3164 - val_accuracy: 0.8846\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1578 - accuracy: 0.9982 - val_loss: 0.3157 - val_accuracy: 0.8846\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1334 - accuracy: 0.9899 - val_loss: 0.3147 - val_accuracy: 0.8846\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1631 - accuracy: 0.9967 - val_loss: 0.3216 - val_accuracy: 0.9038\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1284 - accuracy: 0.9946 - val_loss: 0.3104 - val_accuracy: 0.8846\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1302 - accuracy: 0.9988 - val_loss: 0.3099 - val_accuracy: 0.8846\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1340 - accuracy: 1.0000 - val_loss: 0.3133 - val_accuracy: 0.8846\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1353 - accuracy: 0.9828 - val_loss: 0.3110 - val_accuracy: 0.9038\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1237 - accuracy: 0.9957 - val_loss: 0.3093 - val_accuracy: 0.8846\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1249 - accuracy: 1.0000 - val_loss: 0.3106 - val_accuracy: 0.9038\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1315 - accuracy: 1.0000 - val_loss: 0.3090 - val_accuracy: 0.8846\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1231 - accuracy: 1.0000 - val_loss: 0.3099 - val_accuracy: 0.9038\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1141 - accuracy: 1.0000 - val_loss: 0.3093 - val_accuracy: 0.9038\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1204 - accuracy: 1.0000 - val_loss: 0.3088 - val_accuracy: 0.8846\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1252 - accuracy: 1.0000 - val_loss: 0.3081 - val_accuracy: 0.8846\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1105 - accuracy: 1.0000 - val_loss: 0.3096 - val_accuracy: 0.9038\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1078 - accuracy: 0.9975 - val_loss: 0.3088 - val_accuracy: 0.9038\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1078 - accuracy: 1.0000 - val_loss: 0.3076 - val_accuracy: 0.8846\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1050 - accuracy: 1.0000 - val_loss: 0.3076 - val_accuracy: 0.8846\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1097 - accuracy: 1.0000 - val_loss: 0.3096 - val_accuracy: 0.9038\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1106 - accuracy: 1.0000 - val_loss: 0.3076 - val_accuracy: 0.8846\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1130 - accuracy: 1.0000 - val_loss: 0.3093 - val_accuracy: 0.9038\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1019 - accuracy: 1.0000 - val_loss: 0.3087 - val_accuracy: 0.9038\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1015 - accuracy: 1.0000 - val_loss: 0.3071 - val_accuracy: 0.8846\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1107 - accuracy: 1.0000 - val_loss: 0.3071 - val_accuracy: 0.8846\n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1100 - accuracy: 1.0000 - val_loss: 0.3082 - val_accuracy: 0.9038\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1069 - accuracy: 1.0000 - val_loss: 0.3076 - val_accuracy: 0.9038\n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1069 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.8846\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1135 - accuracy: 1.0000 - val_loss: 0.3084 - val_accuracy: 0.9038\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1069 - accuracy: 1.0000 - val_loss: 0.3070 - val_accuracy: 0.9038\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1060 - accuracy: 1.0000 - val_loss: 0.3075 - val_accuracy: 0.9038\n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0963 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1053 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9038\n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0993 - accuracy: 1.0000 - val_loss: 0.3071 - val_accuracy: 0.9038\n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0959 - accuracy: 1.0000 - val_loss: 0.3075 - val_accuracy: 0.9038\n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0955 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1003 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1001 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 62/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.0992 - accuracy: 1.0000 - val_loss: 0.3072 - val_accuracy: 0.9038\n",
      "Epoch 63/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1030 - accuracy: 1.0000 - val_loss: 0.3070 - val_accuracy: 0.9038\n",
      "Epoch 64/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0898 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9038\n",
      "Epoch 65/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1042 - accuracy: 1.0000 - val_loss: 0.3073 - val_accuracy: 0.9038\n",
      "Epoch 66/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1107 - accuracy: 1.0000 - val_loss: 0.3067 - val_accuracy: 0.9038\n",
      "Epoch 67/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1008 - accuracy: 1.0000 - val_loss: 0.3071 - val_accuracy: 0.9038\n",
      "Epoch 68/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0885 - accuracy: 1.0000 - val_loss: 0.3066 - val_accuracy: 0.9038\n",
      "Epoch 69/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0901 - accuracy: 1.0000 - val_loss: 0.3070 - val_accuracy: 0.9038\n",
      "Epoch 70/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1097 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9038\n",
      "Epoch 71/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1008 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9038\n",
      "Epoch 72/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1020 - accuracy: 1.0000 - val_loss: 0.3070 - val_accuracy: 0.9038\n",
      "Epoch 73/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1058 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 74/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0964 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9038\n",
      "Epoch 75/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1019 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 76/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0904 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 77/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1048 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9038\n",
      "Epoch 78/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0940 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 79/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1068 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 80/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0993 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 81/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0980 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9038\n",
      "Epoch 82/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0955 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9038\n",
      "Epoch 83/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1102 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 84/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1008 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 85/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1010 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 86/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0913 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9038\n",
      "Epoch 87/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0962 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9038\n",
      "Epoch 88/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0987 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9038\n",
      "------------------------------------------------------------------------\n",
      "Score for fold 3: loss of 0.62; accuracy of 71.15%\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4/6 ...\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/300\n",
      "10/10 [==============================] - 16s 550ms/step - loss: 1.0254 - accuracy: 0.7214 - val_loss: 0.6668 - val_accuracy: 0.7115\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.7833 - accuracy: 0.8541 - val_loss: 0.6038 - val_accuracy: 0.8077\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.6381 - accuracy: 0.8777 - val_loss: 0.6784 - val_accuracy: 0.6923\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.6062 - accuracy: 0.8764 - val_loss: 0.4425 - val_accuracy: 0.8654\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.5074 - accuracy: 0.9090 - val_loss: 0.5832 - val_accuracy: 0.7308\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.4647 - accuracy: 0.9090 - val_loss: 0.3418 - val_accuracy: 0.9423\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.3892 - accuracy: 0.9315 - val_loss: 0.3035 - val_accuracy: 0.9423\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.3194 - accuracy: 0.9650 - val_loss: 0.3475 - val_accuracy: 0.9231\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 2s 219ms/step - loss: 0.3234 - accuracy: 0.9780 - val_loss: 0.2864 - val_accuracy: 0.9615\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.3007 - accuracy: 0.9555 - val_loss: 0.2982 - val_accuracy: 0.9423\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 2s 219ms/step - loss: 0.2769 - accuracy: 0.9816 - val_loss: 0.2869 - val_accuracy: 0.9615\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.2972 - accuracy: 0.9419 - val_loss: 0.2793 - val_accuracy: 0.9231\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.2838 - accuracy: 0.9574 - val_loss: 0.4017 - val_accuracy: 0.8654\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 2s 219ms/step - loss: 0.3420 - accuracy: 0.9213 - val_loss: 0.2577 - val_accuracy: 0.9423\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.2666 - accuracy: 0.9802 - val_loss: 0.2701 - val_accuracy: 0.9423\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 2s 220ms/step - loss: 0.2179 - accuracy: 0.9872 - val_loss: 0.2637 - val_accuracy: 0.9423\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.2328 - accuracy: 0.9752 - val_loss: 0.2571 - val_accuracy: 0.9615\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.2110 - accuracy: 0.9685 - val_loss: 0.2674 - val_accuracy: 0.9231\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.2169 - accuracy: 0.9729 - val_loss: 0.2657 - val_accuracy: 0.9038\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.2178 - accuracy: 0.9812 - val_loss: 0.2481 - val_accuracy: 0.9423\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1965 - accuracy: 0.9691 - val_loss: 0.2367 - val_accuracy: 0.9615\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1682 - accuracy: 0.9868 - val_loss: 0.2416 - val_accuracy: 0.9615\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.1696 - accuracy: 0.9859 - val_loss: 0.2371 - val_accuracy: 0.9615\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 2s 207ms/step - loss: 0.2059 - accuracy: 0.9767 - val_loss: 0.2300 - val_accuracy: 0.9615\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1879 - accuracy: 0.9622 - val_loss: 0.2395 - val_accuracy: 0.9615\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1913 - accuracy: 0.9870 - val_loss: 0.2356 - val_accuracy: 0.9423\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1696 - accuracy: 0.9769 - val_loss: 0.2287 - val_accuracy: 0.9615\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1884 - accuracy: 0.9803 - val_loss: 0.2262 - val_accuracy: 0.9615\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 2s 205ms/step - loss: 0.1570 - accuracy: 0.9852 - val_loss: 0.2272 - val_accuracy: 0.9615\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1457 - accuracy: 0.9967 - val_loss: 0.2262 - val_accuracy: 0.9615\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1450 - accuracy: 0.9932 - val_loss: 0.2223 - val_accuracy: 0.9615\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1335 - accuracy: 1.0000 - val_loss: 0.2226 - val_accuracy: 0.9615\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.1480 - accuracy: 0.9988 - val_loss: 0.2176 - val_accuracy: 0.9615\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1395 - accuracy: 1.0000 - val_loss: 0.2264 - val_accuracy: 0.9615\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1334 - accuracy: 0.9982 - val_loss: 0.2287 - val_accuracy: 0.9231\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1342 - accuracy: 1.0000 - val_loss: 0.2179 - val_accuracy: 0.9615\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1340 - accuracy: 1.0000 - val_loss: 0.2160 - val_accuracy: 0.9615\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1197 - accuracy: 0.9932 - val_loss: 0.2327 - val_accuracy: 0.9038\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1223 - accuracy: 1.0000 - val_loss: 0.2163 - val_accuracy: 0.9615\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.1312 - accuracy: 1.0000 - val_loss: 0.2116 - val_accuracy: 0.9615\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.1178 - accuracy: 1.0000 - val_loss: 0.2196 - val_accuracy: 0.9231\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1121 - accuracy: 1.0000 - val_loss: 0.2111 - val_accuracy: 0.9615\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1127 - accuracy: 1.0000 - val_loss: 0.2105 - val_accuracy: 0.9615\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0949 - accuracy: 1.0000 - val_loss: 0.2246 - val_accuracy: 0.9038\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1172 - accuracy: 1.0000 - val_loss: 0.2139 - val_accuracy: 0.9615\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.1320 - accuracy: 1.0000 - val_loss: 0.2081 - val_accuracy: 0.9615\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1108 - accuracy: 1.0000 - val_loss: 0.2088 - val_accuracy: 0.9615\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1047 - accuracy: 1.0000 - val_loss: 0.2108 - val_accuracy: 0.9615\n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.1046 - accuracy: 1.0000 - val_loss: 0.2059 - val_accuracy: 0.9615\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0986 - accuracy: 1.0000 - val_loss: 0.2059 - val_accuracy: 0.9615\n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0997 - accuracy: 1.0000 - val_loss: 0.2067 - val_accuracy: 0.9615\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0902 - accuracy: 1.0000 - val_loss: 0.2046 - val_accuracy: 0.9615\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0964 - accuracy: 1.0000 - val_loss: 0.2054 - val_accuracy: 0.9615\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0923 - accuracy: 1.0000 - val_loss: 0.2036 - val_accuracy: 0.9615\n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.0959 - accuracy: 1.0000 - val_loss: 0.2034 - val_accuracy: 0.9615\n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0856 - accuracy: 1.0000 - val_loss: 0.2056 - val_accuracy: 0.9423\n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0791 - accuracy: 1.0000 - val_loss: 0.2034 - val_accuracy: 0.9615\n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0959 - accuracy: 1.0000 - val_loss: 0.2014 - val_accuracy: 0.9615\n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0781 - accuracy: 1.0000 - val_loss: 0.2018 - val_accuracy: 0.9615\n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0900 - accuracy: 1.0000 - val_loss: 0.2019 - val_accuracy: 0.9615\n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0912 - accuracy: 1.0000 - val_loss: 0.2048 - val_accuracy: 0.9423\n",
      "Epoch 62/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0876 - accuracy: 1.0000 - val_loss: 0.1998 - val_accuracy: 0.9615\n",
      "Epoch 63/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0904 - accuracy: 1.0000 - val_loss: 0.2003 - val_accuracy: 0.9615\n",
      "Epoch 64/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0810 - accuracy: 1.0000 - val_loss: 0.2009 - val_accuracy: 0.9615\n",
      "Epoch 65/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0919 - accuracy: 1.0000 - val_loss: 0.2016 - val_accuracy: 0.9615\n",
      "Epoch 66/300\n",
      "10/10 [==============================] - 2s 207ms/step - loss: 0.0935 - accuracy: 1.0000 - val_loss: 0.1988 - val_accuracy: 0.9615\n",
      "Epoch 67/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0815 - accuracy: 1.0000 - val_loss: 0.2005 - val_accuracy: 0.9615\n",
      "Epoch 68/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0670 - accuracy: 1.0000 - val_loss: 0.2007 - val_accuracy: 0.9423\n",
      "Epoch 69/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0818 - accuracy: 1.0000 - val_loss: 0.1998 - val_accuracy: 0.9615\n",
      "Epoch 70/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0712 - accuracy: 1.0000 - val_loss: 0.1981 - val_accuracy: 0.9615\n",
      "Epoch 71/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0790 - accuracy: 1.0000 - val_loss: 0.2000 - val_accuracy: 0.9615\n",
      "Epoch 72/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0745 - accuracy: 1.0000 - val_loss: 0.2000 - val_accuracy: 0.9423\n",
      "Epoch 73/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0735 - accuracy: 1.0000 - val_loss: 0.1976 - val_accuracy: 0.9615\n",
      "Epoch 74/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0676 - accuracy: 1.0000 - val_loss: 0.1972 - val_accuracy: 0.9615\n",
      "Epoch 75/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0734 - accuracy: 1.0000 - val_loss: 0.1979 - val_accuracy: 0.9615\n",
      "Epoch 76/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0668 - accuracy: 1.0000 - val_loss: 0.1968 - val_accuracy: 0.9615\n",
      "Epoch 77/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0695 - accuracy: 1.0000 - val_loss: 0.1966 - val_accuracy: 0.9615\n",
      "Epoch 78/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0634 - accuracy: 1.0000 - val_loss: 0.1967 - val_accuracy: 0.9615\n",
      "Epoch 79/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0655 - accuracy: 1.0000 - val_loss: 0.1970 - val_accuracy: 0.9615\n",
      "Epoch 80/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0718 - accuracy: 1.0000 - val_loss: 0.1962 - val_accuracy: 0.9615\n",
      "Epoch 81/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0643 - accuracy: 1.0000 - val_loss: 0.1956 - val_accuracy: 0.9615\n",
      "Epoch 82/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0635 - accuracy: 1.0000 - val_loss: 0.1959 - val_accuracy: 0.9615\n",
      "Epoch 83/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0642 - accuracy: 1.0000 - val_loss: 0.1956 - val_accuracy: 0.9615\n",
      "Epoch 84/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0702 - accuracy: 1.0000 - val_loss: 0.1949 - val_accuracy: 0.9615\n",
      "Epoch 85/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0641 - accuracy: 1.0000 - val_loss: 0.1950 - val_accuracy: 0.9615\n",
      "Epoch 86/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0614 - accuracy: 1.0000 - val_loss: 0.1957 - val_accuracy: 0.9615\n",
      "Epoch 87/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0634 - accuracy: 1.0000 - val_loss: 0.1954 - val_accuracy: 0.9423\n",
      "Epoch 88/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0646 - accuracy: 1.0000 - val_loss: 0.1953 - val_accuracy: 0.9615\n",
      "Epoch 89/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0602 - accuracy: 1.0000 - val_loss: 0.1952 - val_accuracy: 0.9615\n",
      "Epoch 90/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0556 - accuracy: 1.0000 - val_loss: 0.1956 - val_accuracy: 0.9615\n",
      "Epoch 91/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0650 - accuracy: 1.0000 - val_loss: 0.1941 - val_accuracy: 0.9615\n",
      "Epoch 92/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0667 - accuracy: 1.0000 - val_loss: 0.1952 - val_accuracy: 0.9615\n",
      "Epoch 93/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0564 - accuracy: 1.0000 - val_loss: 0.1943 - val_accuracy: 0.9615\n",
      "Epoch 94/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0593 - accuracy: 1.0000 - val_loss: 0.1940 - val_accuracy: 0.9615\n",
      "Epoch 95/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0635 - accuracy: 1.0000 - val_loss: 0.1945 - val_accuracy: 0.9615\n",
      "Epoch 96/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0506 - accuracy: 1.0000 - val_loss: 0.1939 - val_accuracy: 0.9615\n",
      "Epoch 97/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0603 - accuracy: 1.0000 - val_loss: 0.1945 - val_accuracy: 0.9615\n",
      "Epoch 98/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0533 - accuracy: 1.0000 - val_loss: 0.1941 - val_accuracy: 0.9615\n",
      "Epoch 99/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0537 - accuracy: 1.0000 - val_loss: 0.1942 - val_accuracy: 0.9615\n",
      "Epoch 100/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.0537 - accuracy: 1.0000 - val_loss: 0.1939 - val_accuracy: 0.9615\n",
      "Epoch 101/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0550 - accuracy: 1.0000 - val_loss: 0.1935 - val_accuracy: 0.9615\n",
      "Epoch 102/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.0501 - accuracy: 1.0000 - val_loss: 0.1937 - val_accuracy: 0.9615\n",
      "Epoch 103/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0579 - accuracy: 1.0000 - val_loss: 0.1941 - val_accuracy: 0.9615\n",
      "Epoch 104/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0522 - accuracy: 1.0000 - val_loss: 0.1939 - val_accuracy: 0.9615\n",
      "Epoch 105/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.0589 - accuracy: 1.0000 - val_loss: 0.1939 - val_accuracy: 0.9615\n",
      "Epoch 106/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.0605 - accuracy: 1.0000 - val_loss: 0.1935 - val_accuracy: 0.9615\n",
      "Epoch 107/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0588 - accuracy: 1.0000 - val_loss: 0.1938 - val_accuracy: 0.9615\n",
      "Epoch 108/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0669 - accuracy: 1.0000 - val_loss: 0.1937 - val_accuracy: 0.9615\n",
      "Epoch 109/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0503 - accuracy: 1.0000 - val_loss: 0.1935 - val_accuracy: 0.9615\n",
      "Epoch 110/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0562 - accuracy: 1.0000 - val_loss: 0.1934 - val_accuracy: 0.9615\n",
      "Epoch 111/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0485 - accuracy: 1.0000 - val_loss: 0.1939 - val_accuracy: 0.9615\n",
      "Epoch 112/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0509 - accuracy: 1.0000 - val_loss: 0.1937 - val_accuracy: 0.9615\n",
      "Epoch 113/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0443 - accuracy: 1.0000 - val_loss: 0.1934 - val_accuracy: 0.9615\n",
      "Epoch 114/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0496 - accuracy: 1.0000 - val_loss: 0.1935 - val_accuracy: 0.9615\n",
      "Epoch 115/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0600 - accuracy: 1.0000 - val_loss: 0.1938 - val_accuracy: 0.9615\n",
      "Epoch 116/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0555 - accuracy: 1.0000 - val_loss: 0.1934 - val_accuracy: 0.9615\n",
      "Epoch 117/300\n",
      "10/10 [==============================] - 2s 220ms/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 0.1934 - val_accuracy: 0.9615\n",
      "Epoch 118/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0494 - accuracy: 1.0000 - val_loss: 0.1932 - val_accuracy: 0.9615\n",
      "Epoch 119/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0466 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 120/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0571 - accuracy: 1.0000 - val_loss: 0.1934 - val_accuracy: 0.9615\n",
      "Epoch 121/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 0.1935 - val_accuracy: 0.9615\n",
      "Epoch 122/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0549 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 123/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0503 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 124/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0529 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 125/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 0.1934 - val_accuracy: 0.9615\n",
      "Epoch 126/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0499 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 127/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 128/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0578 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 129/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 0.1934 - val_accuracy: 0.9615\n",
      "Epoch 130/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0521 - accuracy: 1.0000 - val_loss: 0.1934 - val_accuracy: 0.9615\n",
      "Epoch 131/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.0548 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 132/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0494 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 133/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0502 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 134/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0528 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 135/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 136/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0536 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 137/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.0505 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "Epoch 138/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0509 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9615\n",
      "------------------------------------------------------------------------\n",
      "Score for fold 4: loss of 0.65; accuracy of 75.0%\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5/6 ...\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/300\n",
      "10/10 [==============================] - 15s 559ms/step - loss: 1.0031 - accuracy: 0.7692 - val_loss: 0.4890 - val_accuracy: 0.8269\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.7537 - accuracy: 0.8031 - val_loss: 0.4220 - val_accuracy: 0.9423\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.6618 - accuracy: 0.8528 - val_loss: 0.3330 - val_accuracy: 0.9231\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.5621 - accuracy: 0.9218 - val_loss: 0.3032 - val_accuracy: 0.9423\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.5587 - accuracy: 0.9151 - val_loss: 0.2808 - val_accuracy: 0.9423\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.4972 - accuracy: 0.9401 - val_loss: 0.3049 - val_accuracy: 0.9808\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.4530 - accuracy: 0.9282 - val_loss: 0.2501 - val_accuracy: 0.9615\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.3745 - accuracy: 0.9334 - val_loss: 0.2548 - val_accuracy: 0.9615\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.3277 - accuracy: 0.9593 - val_loss: 0.2716 - val_accuracy: 0.9615\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.3094 - accuracy: 0.9593 - val_loss: 0.2357 - val_accuracy: 0.9615\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.2703 - accuracy: 0.9728 - val_loss: 0.2371 - val_accuracy: 0.9423\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.2853 - accuracy: 0.9690 - val_loss: 0.2293 - val_accuracy: 0.9808\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 2s 219ms/step - loss: 0.2641 - accuracy: 0.9809 - val_loss: 0.2162 - val_accuracy: 0.9615\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.2491 - accuracy: 0.9555 - val_loss: 0.2561 - val_accuracy: 0.8846\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.2888 - accuracy: 0.9319 - val_loss: 0.2182 - val_accuracy: 0.9231\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 2s 220ms/step - loss: 0.2348 - accuracy: 0.9741 - val_loss: 0.2173 - val_accuracy: 0.9808\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.2434 - accuracy: 0.9758 - val_loss: 0.2312 - val_accuracy: 0.8846\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.2058 - accuracy: 0.9756 - val_loss: 0.2169 - val_accuracy: 0.9423\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.2110 - accuracy: 0.9731 - val_loss: 0.2018 - val_accuracy: 0.9615\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.1809 - accuracy: 0.9907 - val_loss: 0.2004 - val_accuracy: 0.9615\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.1743 - accuracy: 0.9946 - val_loss: 0.2003 - val_accuracy: 0.9615\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.2062 - accuracy: 0.9648 - val_loss: 0.2064 - val_accuracy: 0.9615\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.2165 - accuracy: 0.9642 - val_loss: 0.1951 - val_accuracy: 0.9615\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1723 - accuracy: 0.9913 - val_loss: 0.1991 - val_accuracy: 0.9615\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.1743 - accuracy: 0.9913 - val_loss: 0.1958 - val_accuracy: 0.9808\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1651 - accuracy: 0.9889 - val_loss: 0.1961 - val_accuracy: 0.9615\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1830 - accuracy: 0.9888 - val_loss: 0.1936 - val_accuracy: 0.9615\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1817 - accuracy: 0.9795 - val_loss: 0.1931 - val_accuracy: 0.9615\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1747 - accuracy: 0.9932 - val_loss: 0.1946 - val_accuracy: 0.9615\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1416 - accuracy: 0.9915 - val_loss: 0.1887 - val_accuracy: 0.9808\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1588 - accuracy: 0.9949 - val_loss: 0.1914 - val_accuracy: 0.9615\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1769 - accuracy: 0.9913 - val_loss: 0.1956 - val_accuracy: 0.9615\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.1629 - accuracy: 0.9859 - val_loss: 0.1872 - val_accuracy: 0.9808\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1832 - accuracy: 0.9932 - val_loss: 0.1906 - val_accuracy: 0.9615\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1557 - accuracy: 0.9867 - val_loss: 0.1869 - val_accuracy: 0.9615\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1500 - accuracy: 0.9988 - val_loss: 0.1938 - val_accuracy: 0.9615\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1576 - accuracy: 0.9880 - val_loss: 0.1831 - val_accuracy: 0.9808\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1603 - accuracy: 0.9988 - val_loss: 0.1931 - val_accuracy: 0.9615\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1494 - accuracy: 0.9867 - val_loss: 0.1850 - val_accuracy: 0.9615\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1442 - accuracy: 0.9967 - val_loss: 0.1828 - val_accuracy: 0.9615\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1608 - accuracy: 0.9957 - val_loss: 0.1844 - val_accuracy: 0.9615\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.1409 - accuracy: 0.9967 - val_loss: 0.1805 - val_accuracy: 0.9615\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.1405 - accuracy: 0.9975 - val_loss: 0.1821 - val_accuracy: 0.9615\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 2s 219ms/step - loss: 0.1523 - accuracy: 0.9932 - val_loss: 0.1807 - val_accuracy: 0.9615\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.1450 - accuracy: 0.9913 - val_loss: 0.1836 - val_accuracy: 0.9615\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1345 - accuracy: 0.9975 - val_loss: 0.1789 - val_accuracy: 0.9615\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1188 - accuracy: 1.0000 - val_loss: 0.1792 - val_accuracy: 0.9615\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1229 - accuracy: 0.9967 - val_loss: 0.1776 - val_accuracy: 0.9615\n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.1311 - accuracy: 1.0000 - val_loss: 0.1813 - val_accuracy: 0.9615\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1411 - accuracy: 0.9828 - val_loss: 0.1792 - val_accuracy: 0.9615\n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1206 - accuracy: 0.9967 - val_loss: 0.1774 - val_accuracy: 0.9615\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.1370 - accuracy: 1.0000 - val_loss: 0.1789 - val_accuracy: 0.9615\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1315 - accuracy: 0.9957 - val_loss: 0.1775 - val_accuracy: 0.9615\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 2s 219ms/step - loss: 0.1260 - accuracy: 0.9982 - val_loss: 0.1760 - val_accuracy: 0.9615\n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.1374 - accuracy: 1.0000 - val_loss: 0.1740 - val_accuracy: 0.9615\n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1195 - accuracy: 1.0000 - val_loss: 0.1755 - val_accuracy: 0.9615\n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1337 - accuracy: 1.0000 - val_loss: 0.1756 - val_accuracy: 0.9615\n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1214 - accuracy: 1.0000 - val_loss: 0.1745 - val_accuracy: 0.9615\n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.1321 - accuracy: 1.0000 - val_loss: 0.1770 - val_accuracy: 0.9615\n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1025 - accuracy: 1.0000 - val_loss: 0.1725 - val_accuracy: 0.9615\n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1165 - accuracy: 1.0000 - val_loss: 0.1749 - val_accuracy: 0.9615\n",
      "Epoch 62/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1105 - accuracy: 0.9982 - val_loss: 0.1749 - val_accuracy: 0.9615\n",
      "Epoch 63/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.1056 - accuracy: 1.0000 - val_loss: 0.1718 - val_accuracy: 0.9615\n",
      "Epoch 64/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1187 - accuracy: 1.0000 - val_loss: 0.1730 - val_accuracy: 0.9615\n",
      "Epoch 65/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.1002 - accuracy: 1.0000 - val_loss: 0.1710 - val_accuracy: 0.9615\n",
      "Epoch 66/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1130 - accuracy: 0.9982 - val_loss: 0.1732 - val_accuracy: 0.9615\n",
      "Epoch 67/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.1174 - accuracy: 1.0000 - val_loss: 0.1695 - val_accuracy: 0.9615\n",
      "Epoch 68/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1216 - accuracy: 1.0000 - val_loss: 0.1731 - val_accuracy: 0.9615\n",
      "Epoch 69/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0963 - accuracy: 1.0000 - val_loss: 0.1701 - val_accuracy: 0.9615\n",
      "Epoch 70/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1119 - accuracy: 1.0000 - val_loss: 0.1728 - val_accuracy: 0.9615\n",
      "Epoch 71/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0905 - accuracy: 1.0000 - val_loss: 0.1682 - val_accuracy: 0.9615\n",
      "Epoch 72/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1130 - accuracy: 1.0000 - val_loss: 0.1702 - val_accuracy: 0.9615\n",
      "Epoch 73/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1078 - accuracy: 1.0000 - val_loss: 0.1695 - val_accuracy: 0.9615\n",
      "Epoch 74/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0959 - accuracy: 1.0000 - val_loss: 0.1696 - val_accuracy: 0.9615\n",
      "Epoch 75/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1013 - accuracy: 1.0000 - val_loss: 0.1666 - val_accuracy: 0.9615\n",
      "Epoch 76/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0989 - accuracy: 1.0000 - val_loss: 0.1684 - val_accuracy: 0.9615\n",
      "Epoch 77/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1010 - accuracy: 1.0000 - val_loss: 0.1682 - val_accuracy: 0.9615\n",
      "Epoch 78/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0940 - accuracy: 1.0000 - val_loss: 0.1677 - val_accuracy: 0.9615\n",
      "Epoch 79/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.1021 - accuracy: 1.0000 - val_loss: 0.1665 - val_accuracy: 0.9615\n",
      "Epoch 80/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.1004 - accuracy: 1.0000 - val_loss: 0.1690 - val_accuracy: 0.9615\n",
      "Epoch 81/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1084 - accuracy: 1.0000 - val_loss: 0.1690 - val_accuracy: 0.9615\n",
      "Epoch 82/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1050 - accuracy: 1.0000 - val_loss: 0.1633 - val_accuracy: 0.9808\n",
      "Epoch 83/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0950 - accuracy: 1.0000 - val_loss: 0.1683 - val_accuracy: 0.9615\n",
      "Epoch 84/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0930 - accuracy: 1.0000 - val_loss: 0.1648 - val_accuracy: 0.9615\n",
      "Epoch 85/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0984 - accuracy: 1.0000 - val_loss: 0.1655 - val_accuracy: 0.9615\n",
      "Epoch 86/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0915 - accuracy: 1.0000 - val_loss: 0.1651 - val_accuracy: 0.9615\n",
      "Epoch 87/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0861 - accuracy: 1.0000 - val_loss: 0.1665 - val_accuracy: 0.9615\n",
      "Epoch 88/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.1009 - accuracy: 1.0000 - val_loss: 0.1640 - val_accuracy: 0.9615\n",
      "Epoch 89/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0830 - accuracy: 1.0000 - val_loss: 0.1639 - val_accuracy: 0.9615\n",
      "Epoch 90/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0921 - accuracy: 1.0000 - val_loss: 0.1665 - val_accuracy: 0.9615\n",
      "Epoch 91/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0862 - accuracy: 1.0000 - val_loss: 0.1641 - val_accuracy: 0.9615\n",
      "Epoch 92/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0911 - accuracy: 1.0000 - val_loss: 0.1642 - val_accuracy: 0.9615\n",
      "Epoch 93/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.0937 - accuracy: 1.0000 - val_loss: 0.1640 - val_accuracy: 0.9615\n",
      "Epoch 94/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0851 - accuracy: 1.0000 - val_loss: 0.1639 - val_accuracy: 0.9615\n",
      "Epoch 95/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0888 - accuracy: 1.0000 - val_loss: 0.1643 - val_accuracy: 0.9615\n",
      "Epoch 96/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0858 - accuracy: 1.0000 - val_loss: 0.1640 - val_accuracy: 0.9615\n",
      "Epoch 97/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0830 - accuracy: 1.0000 - val_loss: 0.1641 - val_accuracy: 0.9615\n",
      "Epoch 98/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0956 - accuracy: 1.0000 - val_loss: 0.1637 - val_accuracy: 0.9615\n",
      "Epoch 99/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0930 - accuracy: 1.0000 - val_loss: 0.1637 - val_accuracy: 0.9615\n",
      "Epoch 100/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0929 - accuracy: 1.0000 - val_loss: 0.1639 - val_accuracy: 0.9615\n",
      "Epoch 101/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0890 - accuracy: 1.0000 - val_loss: 0.1638 - val_accuracy: 0.9615\n",
      "Epoch 102/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0926 - accuracy: 1.0000 - val_loss: 0.1640 - val_accuracy: 0.9615\n",
      "------------------------------------------------------------------------\n",
      "Score for fold 5: loss of 0.64; accuracy of 71.15%\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6/6 ...\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/300\n",
      "10/10 [==============================] - 15s 559ms/step - loss: 0.7925 - accuracy: 0.8811 - val_loss: 1.1846 - val_accuracy: 0.3846\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.8149 - accuracy: 0.7744 - val_loss: 0.7320 - val_accuracy: 0.6346\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.5275 - accuracy: 0.9548 - val_loss: 0.7196 - val_accuracy: 0.6731\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.4479 - accuracy: 0.9414 - val_loss: 0.7164 - val_accuracy: 0.6731\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.3858 - accuracy: 0.9245 - val_loss: 0.6117 - val_accuracy: 0.7692\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.3109 - accuracy: 0.9914 - val_loss: 0.6229 - val_accuracy: 0.7308\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.3071 - accuracy: 0.9861 - val_loss: 0.5687 - val_accuracy: 0.7692\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.3335 - accuracy: 0.9307 - val_loss: 0.5664 - val_accuracy: 0.7692\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 2s 219ms/step - loss: 0.2967 - accuracy: 0.9673 - val_loss: 0.5574 - val_accuracy: 0.8077\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.2260 - accuracy: 0.9885 - val_loss: 0.5624 - val_accuracy: 0.7885\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 2s 220ms/step - loss: 0.2383 - accuracy: 0.9774 - val_loss: 0.5473 - val_accuracy: 0.8269\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.2361 - accuracy: 0.9888 - val_loss: 0.5358 - val_accuracy: 0.8269\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 2s 222ms/step - loss: 0.1864 - accuracy: 0.9975 - val_loss: 0.5359 - val_accuracy: 0.8077\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 2s 219ms/step - loss: 0.2193 - accuracy: 0.9828 - val_loss: 0.5358 - val_accuracy: 0.8077\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 2s 220ms/step - loss: 0.1933 - accuracy: 0.9946 - val_loss: 0.5258 - val_accuracy: 0.8269\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 2s 222ms/step - loss: 0.1705 - accuracy: 0.9933 - val_loss: 0.5736 - val_accuracy: 0.7500\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.2107 - accuracy: 0.9783 - val_loss: 0.5226 - val_accuracy: 0.8077\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 2s 222ms/step - loss: 0.1634 - accuracy: 1.0000 - val_loss: 0.5206 - val_accuracy: 0.8077\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.1629 - accuracy: 1.0000 - val_loss: 0.5163 - val_accuracy: 0.8462\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.1648 - accuracy: 0.9932 - val_loss: 0.5152 - val_accuracy: 0.8077\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.1446 - accuracy: 0.9864 - val_loss: 0.5166 - val_accuracy: 0.8269\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1424 - accuracy: 1.0000 - val_loss: 0.5161 - val_accuracy: 0.8462\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.1441 - accuracy: 1.0000 - val_loss: 0.5096 - val_accuracy: 0.8269\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.1202 - accuracy: 1.0000 - val_loss: 0.5115 - val_accuracy: 0.8077\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 2s 220ms/step - loss: 0.1274 - accuracy: 1.0000 - val_loss: 0.5088 - val_accuracy: 0.8269\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1229 - accuracy: 1.0000 - val_loss: 0.5088 - val_accuracy: 0.8462\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1329 - accuracy: 1.0000 - val_loss: 0.5084 - val_accuracy: 0.8462\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1134 - accuracy: 0.9957 - val_loss: 0.5084 - val_accuracy: 0.8269\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1139 - accuracy: 1.0000 - val_loss: 0.5137 - val_accuracy: 0.8077\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 2s 209ms/step - loss: 0.1069 - accuracy: 1.0000 - val_loss: 0.5113 - val_accuracy: 0.8269\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.1172 - accuracy: 1.0000 - val_loss: 0.5092 - val_accuracy: 0.8269\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 2s 219ms/step - loss: 0.1097 - accuracy: 1.0000 - val_loss: 0.5133 - val_accuracy: 0.8077\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.1244 - accuracy: 0.9946 - val_loss: 0.5081 - val_accuracy: 0.8269\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0979 - accuracy: 1.0000 - val_loss: 0.5073 - val_accuracy: 0.8269\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1028 - accuracy: 1.0000 - val_loss: 0.5086 - val_accuracy: 0.8269\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0904 - accuracy: 1.0000 - val_loss: 0.5076 - val_accuracy: 0.8269\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0973 - accuracy: 1.0000 - val_loss: 0.5073 - val_accuracy: 0.8269\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0935 - accuracy: 1.0000 - val_loss: 0.5074 - val_accuracy: 0.8269\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.0908 - accuracy: 1.0000 - val_loss: 0.5069 - val_accuracy: 0.8462\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0904 - accuracy: 1.0000 - val_loss: 0.5067 - val_accuracy: 0.8462\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.1077 - accuracy: 1.0000 - val_loss: 0.5087 - val_accuracy: 0.8269\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0965 - accuracy: 1.0000 - val_loss: 0.5074 - val_accuracy: 0.8269\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0941 - accuracy: 1.0000 - val_loss: 0.5074 - val_accuracy: 0.8269\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0891 - accuracy: 1.0000 - val_loss: 0.5076 - val_accuracy: 0.8269\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0888 - accuracy: 1.0000 - val_loss: 0.5065 - val_accuracy: 0.8462\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0987 - accuracy: 1.0000 - val_loss: 0.5076 - val_accuracy: 0.8269\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 2s 219ms/step - loss: 0.0951 - accuracy: 1.0000 - val_loss: 0.5076 - val_accuracy: 0.8269\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0818 - accuracy: 1.0000 - val_loss: 0.5071 - val_accuracy: 0.8462\n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0855 - accuracy: 1.0000 - val_loss: 0.5089 - val_accuracy: 0.8269\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0924 - accuracy: 1.0000 - val_loss: 0.5070 - val_accuracy: 0.8462\n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 2s 210ms/step - loss: 0.0976 - accuracy: 1.0000 - val_loss: 0.5095 - val_accuracy: 0.8269\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.0837 - accuracy: 1.0000 - val_loss: 0.5090 - val_accuracy: 0.8269\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0843 - accuracy: 1.0000 - val_loss: 0.5079 - val_accuracy: 0.8269\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0828 - accuracy: 1.0000 - val_loss: 0.5085 - val_accuracy: 0.8269\n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 2s 220ms/step - loss: 0.0728 - accuracy: 1.0000 - val_loss: 0.5080 - val_accuracy: 0.8269\n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 2s 219ms/step - loss: 0.0905 - accuracy: 1.0000 - val_loss: 0.5072 - val_accuracy: 0.8269\n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0916 - accuracy: 1.0000 - val_loss: 0.5078 - val_accuracy: 0.8269\n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 2s 220ms/step - loss: 0.0808 - accuracy: 1.0000 - val_loss: 0.5082 - val_accuracy: 0.8269\n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 2s 220ms/step - loss: 0.0760 - accuracy: 1.0000 - val_loss: 0.5081 - val_accuracy: 0.8269\n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 2s 219ms/step - loss: 0.0793 - accuracy: 1.0000 - val_loss: 0.5081 - val_accuracy: 0.8269\n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0730 - accuracy: 1.0000 - val_loss: 0.5079 - val_accuracy: 0.8269\n",
      "Epoch 62/300\n",
      "10/10 [==============================] - 2s 218ms/step - loss: 0.0887 - accuracy: 1.0000 - val_loss: 0.5076 - val_accuracy: 0.8269\n",
      "Epoch 63/300\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.0719 - accuracy: 1.0000 - val_loss: 0.5077 - val_accuracy: 0.8269\n",
      "Epoch 64/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0864 - accuracy: 1.0000 - val_loss: 0.5080 - val_accuracy: 0.8269\n",
      "Epoch 65/300\n",
      "10/10 [==============================] - 2s 217ms/step - loss: 0.0835 - accuracy: 1.0000 - val_loss: 0.5081 - val_accuracy: 0.8269\n",
      "------------------------------------------------------------------------\n",
      "Score for fold 6: loss of 0.57; accuracy of 71.15%\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6 - Accuracy: 0.71%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.64 - Accuracy: 0.75%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.62 - Accuracy: 0.71%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.65 - Accuracy: 0.75%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.64 - Accuracy: 0.71%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.57 - Accuracy: 0.71%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 0.72 (+- 0.02)\n",
      "> Loss: 0.62 (+- 0.03)\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "history_array = np.array([])\n",
    "scores_array = np.empty([n_folds, 2])\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold + 1}/{n_folds} ...')\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "    # Generate the fold sample for train-test\n",
    "    X_train_cc, X_train_mlo, Y_train, X_test_cc, X_test_mlo, Y_test = part_traintest(X_traintest_cc, X_traintest_mlo, Y_traintest, rand_seed + fold, frac_test)\n",
    "    \n",
    "    # Define the model architecture\n",
    "    model_2ramas = DenseNet_2Ramas(model_cc, model_mlo, rand_seed, learning_rate, momentum)\n",
    "    \n",
    "    # Fit data to model\n",
    "    history = model_2ramas.fit([X_train_cc, X_train_mlo], Y_train,\n",
    "                               batch_size = batch_size,\n",
    "                               epochs = no_epochs,\n",
    "                               validation_data = ([X_test_cc, X_test_mlo], Y_test),\n",
    "                               class_weight = class_weight,\n",
    "                               verbose = 1,\n",
    "                               callbacks = [early_stopping, reduce_lr])\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores_array[fold, :] = model_2ramas.evaluate([X_val_cc, X_val_mlo], Y_val, verbose = 0)\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Score for fold {fold + 1}: {model_2ramas.metrics_names[0]} of {round(scores_array[fold, 0], 2)}; {model_2ramas.metrics_names[1]} of {round(scores_array[fold, 1]*100, 2)}%')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('')\n",
    "    \n",
    "    # Append history callback into array\n",
    "    history_array = np.append(history_array, [history])\n",
    "    \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, scores_array.shape[0]):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i + 1} - Loss: {round(scores_array[i, 0], 2)} - Accuracy: {round(scores_array[i, 1], 2)}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {round(np.mean(scores_array[:, 1]), 2)} (+- {round(np.std(scores_array[:, 1]), 2)})')\n",
    "print(f'> Loss: {round(np.mean(scores_array[:, 0]), 2)} (+- {round(np.std(scores_array[:, 0]), 2)})')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-virginia",
   "metadata": {
    "id": "super-progressive"
   },
   "source": [
    "## Representación de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-faith",
   "metadata": {
    "id": "streaming-recruitment"
   },
   "source": [
    "Definimos una serie de funciones auxiliares para facilitar la visualización de resultados (representación de la evolución de las métricas durante el proceso de entrenamiento y resultados de la matriz de confusión finalmente obtenida)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "positive-facing",
   "metadata": {
    "executionInfo": {
     "elapsed": 1434193,
     "status": "ok",
     "timestamp": 1621190143830,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "published-lingerie"
   },
   "outputs": [],
   "source": [
    "def get_metrics(history_array, no_epochs):\n",
    "    \"\"\"\n",
    "    Function that extracts the metrics from the k-fold history from the training process\n",
    "        - history_array is the array of histories generated during the training process\n",
    "        - no_epochs is number of epochs fixed for the training process\n",
    "    Returns:\n",
    "        - accuracy is the output array of accuracies\n",
    "        - val_accuracy is the output array of validation accuracies\n",
    "        - loss is the output array of losses\n",
    "        - val_loss is the output array of validation losses\n",
    "    \"\"\"\n",
    "    accuracy = np.empty([no_epochs, len(history_array)])\n",
    "    accuracy[:, :] = np.nan\n",
    "    val_accuracy = np.empty([no_epochs, len(history_array)])\n",
    "    val_accuracy[:, :] = np.nan\n",
    "    loss = np.empty([no_epochs, len(history_array)])\n",
    "    loss[:, :] = np.nan\n",
    "    val_loss = np.empty([no_epochs, len(history_array)])\n",
    "    val_loss[:, :] = np.nan\n",
    "\n",
    "    for idx, fold in enumerate(history_array):\n",
    "        max_epochs = max(fold.epoch)\n",
    "        accuracy[:max_epochs + 1, idx] = fold.history['accuracy']\n",
    "        val_accuracy[:max_epochs + 1, idx] = fold.history['val_accuracy']\n",
    "        loss[:max_epochs + 1, idx] = fold.history['loss']\n",
    "        val_loss[:max_epochs + 1, idx] = fold.history['val_loss']\n",
    "\n",
    "    return accuracy, val_accuracy, loss, val_loss\n",
    "\n",
    "def plot_metrics(history_array, no_epochs):\n",
    "    \"\"\"\n",
    "    Function that plots the metrics from the training process\n",
    "        - history_array is the array of histories generated during the training process\n",
    "        - no_epochs is number of epochs fixed for the training process\n",
    "    Returns:\n",
    "        - accuracy is the output array of accuracies\n",
    "        - val_accuracy is the output array of validation accuracies\n",
    "        - loss is the output array of losses\n",
    "        - val_loss is the output array of validation losses\n",
    "    \"\"\"\n",
    "    accuracy, val_accuracy, loss, val_loss = get_metrics(history_array, no_epochs)\n",
    "\n",
    "    fig = plt.figure(figsize = (15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.mean(accuracy, axis = 1))\n",
    "    plt.plot(np.mean(val_accuracy, axis = 1))\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc = 'upper left')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(np.mean(loss, axis = 1))\n",
    "    plt.plot(np.mean(val_loss, axis = 1))\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc = 'upper right')\n",
    "    \n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"\n",
    "    Credit to:\n",
    "    https://gist.github.com/zachguo/10296432\n",
    "    \n",
    "    Function that makes a pretty print for confusion matrixes\n",
    "        - cm is the array of histories generated during the training process\n",
    "        - labels is number of epochs fixed for the training process\n",
    "        - hide_zeroes is a boolean that allows the user to hide the zeroes in the matrix\n",
    "        - hide_diagonal is a boolean that allows the user to hide the diagonal in the matrix\n",
    "        - hide_threshold is a number that allows the user to hide results in the matrix below that value\n",
    "    \"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-carol",
   "metadata": {
    "executionInfo": {
     "elapsed": 2276,
     "status": "ok",
     "timestamp": 1620075964551,
     "user": {
      "displayName": "Duun V",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GipsnLzW9bWFJbKNrxY-F-xi7XKh0HhowJGi0hF9A=s64",
      "userId": "10921869077997462696"
     },
     "user_tz": -120
    },
    "id": "marked-decimal"
   },
   "source": [
    "Visualizamos la evolución de las métricas durante el proceso de entrenamiento de las redes, haciendo la media por época de las distintas iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "partial-candy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 1434738,
     "status": "ok",
     "timestamp": 1621190144383,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "racial-tribute",
    "outputId": "a5234e36-de66-4044-d2d5-2e0943b85ff3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(history_array, no_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-fisher",
   "metadata": {
    "id": "5bXsi3jhTHo_"
   },
   "source": [
    "Buscamos el fold con los mejores resultados sobre el conjunto de validación y lo seleccionamos para representar sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "changed-empire",
   "metadata": {
    "executionInfo": {
     "elapsed": 1434737,
     "status": "ok",
     "timestamp": 1621190144384,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "YV9gGPf5TIDj"
   },
   "outputs": [],
   "source": [
    "idx_best = np.argmax(scores_array[:, 1])\n",
    "model_best = history_array[idx_best].model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-carroll",
   "metadata": {
    "id": "t_0z4ogBWHxS"
   },
   "source": [
    "Mostramos la matriz de confusión de la última iteración por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "continued-inflation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1439505,
     "status": "ok",
     "timestamp": 1621190149154,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "relevant-technique",
    "outputId": "481649b4-0a50-4a1f-d610-1b2c5ab8ebfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            benigno seguimiento  maligno \n",
      "    benigno    24.0         1.0      3.0 \n",
      "seguimiento     3.0         1.0      0.0 \n",
      "    maligno     6.0         0.0     14.0 \n"
     ]
    }
   ],
   "source": [
    "conf_matrix = metrics.confusion_matrix(Y_val.argmax(axis = 1), model_best.predict([X_val_cc, X_val_mlo]).argmax(axis = 1))\n",
    "print_cm(conf_matrix, list(dict_valores.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-skiing",
   "metadata": {
    "id": "viral-constitutional"
   },
   "source": [
    "Mostramos las métricas de resultados según categoría para poder evaluar el desempeño de la red en cada caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "balanced-newcastle",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1439839,
     "status": "ok",
     "timestamp": 1621190149493,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "gross-aaron",
    "outputId": "389d12af-f164-4862-d478-6e54bed931b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     benigno       0.73      0.86      0.79        28\n",
      " seguimiento       0.50      0.25      0.33         4\n",
      "     maligno       0.82      0.70      0.76        20\n",
      "\n",
      "    accuracy                           0.75        52\n",
      "   macro avg       0.68      0.60      0.63        52\n",
      "weighted avg       0.75      0.75      0.74        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classif_report = metrics.classification_report(Y_val.argmax(axis = 1), model_best.predict([X_val_cc, X_val_mlo]).argmax(axis = 1),\n",
    "                                               target_names = list(dict_valores.keys()))\n",
    "print(classif_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-culture",
   "metadata": {
    "id": "heavy-company"
   },
   "source": [
    "Guardamos el modelo entrenado para su uso en pasos posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "recent-calibration",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1542907,
     "status": "ok",
     "timestamp": 1621190252565,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "foreign-teens",
    "outputId": "56283f73-9c46-471f-f97b-e777175e93cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Colab Notebooks/model_CC-MLO/assets\n"
     ]
    }
   ],
   "source": [
    "if google_colab:\n",
    "    file_path = '/content/gdrive/MyDrive/Colab Notebooks/model_CC-MLO'\n",
    "else:\n",
    "    file_path = './model_CC-MLO'\n",
    "K.models.save_model(model_best, file_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Entrenamiento_DenseNet_2Ramas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
