{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exposed-generic",
   "metadata": {
    "id": "modular-forth"
   },
   "source": [
    "# Búsqueda de hiperparámetros para el entrenamiento de DenseNet para las dos vistas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-notion",
   "metadata": {
    "id": "younger-sentence"
   },
   "source": [
    "Ajustamos el notebook según estemos trabajando en local o en un entorno de Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dominican-gauge",
   "metadata": {
    "id": "flying-consciousness"
   },
   "outputs": [],
   "source": [
    "google_colab = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-bradford",
   "metadata": {},
   "source": [
    "Importamos todas las librerías necesarias para la implementación de la búsqueda de hiperparámetros de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pressing-locator",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5748,
     "status": "ok",
     "timestamp": 1621164264052,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "atomic-yukon",
    "outputId": "381db5dd-4276-4e31-b71b-0e6d9fc6e25f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "\n",
    "if google_colab:\n",
    "    from google.colab import drive\n",
    "    !pip install pickle5\n",
    "    import pickle5 as pickle\n",
    "    drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-oracle",
   "metadata": {
    "id": "engaging-persian"
   },
   "source": [
    "##  Carga del dataset y preparación de los dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-virtue",
   "metadata": {
    "id": "beautiful-monthly"
   },
   "source": [
    "Definimos una función auxiliar para ayudar con el preprocesamiento de los datos (ajuste de entrada para la DenseNet en el caso de las imágenes y conversión a one-hot encoding para las etiquetas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recorded-particle",
   "metadata": {
    "id": "traditional-islam"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X_cc, X_mlo, Y):\n",
    "    \"\"\"\n",
    "    Pre-processes the data for the model\n",
    "        - X_cc is a numpy.ndarray of shape (m, 1024, 1024, 3) containing\n",
    "         the CC view mammography, where m is the number of data points\n",
    "        - X_mlo is a numpy.ndarray of shape (m, 1024, 1024, 3) containing\n",
    "         the MLO view mammography, where m is the number of data points\n",
    "        - Y is a numpy.ndarray of shape (m,) containing\n",
    "         the Bi-Rads labels for X\n",
    "    Returns:\n",
    "        - X_cc_p is a numpy.ndarray containing the preprocessed X_cc\n",
    "        - X_mlo_p is a numpy.ndarray containing the preprocessed X_mlo\n",
    "        - Y_p is a numpy.ndarray containing the preprocessed Y\n",
    "    \"\"\"\n",
    "    X_cc_p = K.applications.densenet.preprocess_input(X_cc)\n",
    "    X_mlo_p = K.applications.densenet.preprocess_input(X_mlo)\n",
    "\n",
    "    Y_p = K.utils.to_categorical(Y, 3)\n",
    "\n",
    "    return X_cc_p, X_mlo_p, Y_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-leone",
   "metadata": {
    "id": "mobile-monroe"
   },
   "source": [
    "Cargamos los ficheros de entrada, tanto el de entrenamiento-test como el de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mighty-grant",
   "metadata": {
    "id": "frequent-homework"
   },
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "    with open('/content/gdrive/MyDrive/Colab Notebooks/df_INbreast_train.pkl', 'rb') as pickle_file:\n",
    "        df_INbreast_train = pickle.load(pickle_file)\n",
    "    with open('/content/gdrive/MyDrive/Colab Notebooks/df_INbreast_val.pkl', 'rb') as pickle_file:\n",
    "        df_INbreast_val = pickle.load(pickle_file)\n",
    "else:\n",
    "    df_INbreast_train = pd.read_pickle('./df_INbreast_train.pkl')\n",
    "    df_INbreast_val = pd.read_pickle('./df_INbreast_val.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-pledge",
   "metadata": {
    "id": "contained-franchise"
   },
   "source": [
    "Cargamos los datos, convertimos las etiquetas a enteros y liberamos espacio de los ficheros que contenían el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "taken-easter",
   "metadata": {
    "id": "fatty-forwarding"
   },
   "outputs": [],
   "source": [
    "dict_valores = {'benigno': 0, 'seguimiento': 1, 'maligno': 2}\n",
    "Y_traintest = np.array(df_INbreast_train['Bi-Rads'].map(dict_valores).tolist())\n",
    "X_traintest_cc = np.array(df_INbreast_train['CC Image'].tolist())\n",
    "X_traintest_mlo = np.array(df_INbreast_train['MLO Image'].tolist())\n",
    "Y_val = np.array(df_INbreast_val['Bi-Rads'].map(dict_valores).tolist())\n",
    "X_val_cc = np.array(df_INbreast_val['CC Image'].tolist())\n",
    "X_val_mlo = np.array(df_INbreast_val['MLO Image'].tolist())\n",
    "X_val_cc, X_val_mlo, Y_val = preprocess_data(X_val_cc, X_val_mlo, Y_val)\n",
    "del df_INbreast_train\n",
    "del df_INbreast_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-cincinnati",
   "metadata": {
    "id": "W4IVXDMAOplr"
   },
   "source": [
    "## Definición de la arquitectura de red neuronal de dos ramas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-melissa",
   "metadata": {
    "id": "5moG-NYZcICz"
   },
   "source": [
    "Cargamos los modelos individuales de cada rama y definimos el nuevo modelo a partir de ellos, junto con el inicializador de los pesos de la capa conectada y el optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hazardous-height",
   "metadata": {
    "id": "8Yka6jqyOtSx"
   },
   "outputs": [],
   "source": [
    "model_cc = K.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/model_CC')\n",
    "model_mlo = K.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/model_MLO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "regulated-hospital",
   "metadata": {
    "id": "7UvQIbcNPxqL"
   },
   "outputs": [],
   "source": [
    "def DenseNet_2Ramas(model_cc, model_mlo, rand_seed = 2021, learning_rate = 0.0001, momentum = 0.9):\n",
    "    \"\"\"\n",
    "    Define the DenseNet architecture and compile the model\n",
    "        - model_cc is the pretrained CC-view DenseNet model\n",
    "        - model_mlo is the pretrained MLO-view DenseNet model\n",
    "        - rand_seed is a random seed number used during the fc layer initialization\n",
    "        - learning_rate is the learning rate used during the training\n",
    "        - momentum is the parameters that defines the momentun for the SGD optimizer\n",
    "    Returns:\n",
    "        - model_2ramas is the output compiled DenseNet 2-branch model\n",
    "    \"\"\"\n",
    "    # Define the model architecture\n",
    "    model_cc = K.Sequential(model_cc.layers[:-1])\n",
    "    model_mlo = K.Sequential(model_mlo.layers[:-1])\n",
    "\n",
    "    model_cc.layers[-1]._name = model_cc.layers[-1].name + '_cc'\n",
    "    model_mlo.layers[-1]._name = model_mlo.layers[-1].name + '_mlo'\n",
    "\n",
    "    model_cc.trainable = False\n",
    "    model_mlo.trainable = False\n",
    "\n",
    "    combined = K.layers.Concatenate()([model_cc.output, model_mlo.output])\n",
    "    \n",
    "    initializer = K.initializers.he_normal(seed = rand_seed)\n",
    "    \n",
    "    fc_layer = K.layers.Dense(units = 3,\n",
    "                              activation = 'softmax',\n",
    "                              kernel_initializer = initializer\n",
    "                              )(combined)\n",
    "\n",
    "    model_2ramas = K.models.Model(inputs = [model_cc.input, model_mlo.input], outputs = fc_layer)\n",
    "\n",
    "    # Compile the model\n",
    "    opt = K.optimizers.SGD(learning_rate = learning_rate, momentum = momentum)\n",
    "    \n",
    "    model_2ramas.compile(loss = 'categorical_crossentropy',\n",
    "                         optimizer = opt,\n",
    "                         metrics = ['accuracy'])\n",
    "    \n",
    "    return model_2ramas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-denial",
   "metadata": {
    "id": "LNizgVEZ5ENX"
   },
   "source": [
    "Mostramos por pantalla la arquitectura de la red definida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "electric-editing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117242,
     "status": "ok",
     "timestamp": 1621183449716,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "M_Sbv12gckjn",
    "outputId": "4e23e005-6b09-4247-e326-b538885bdfbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_82 (InputLayer)           [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resize_CC (Lambda)              (None, 256, 256, 3)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "resize_MLO (Lambda)             (None, 256, 256, 3)  0           input_82[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "densenet121_cc (Functional)     (None, 1024)         7037504     resize_CC[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "densenet121_mlo (Functional)    (None, 1024)         7037504     resize_MLO[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2048)         0           densenet121_cc[1][0]             \n",
      "                                                                 densenet121_mlo[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            6147        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 14,081,155\n",
      "Trainable params: 6,147\n",
      "Non-trainable params: 14,075,008\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DenseNet_2Ramas(model_cc, model_mlo).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-matrix",
   "metadata": {
    "id": "daily-secretariat"
   },
   "source": [
    "## Entrenamiento de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-victim",
   "metadata": {
    "id": "FuMQgi36S0eV"
   },
   "source": [
    "Definimos una función auxiliar que particiona el conjunto de entrenamiento/test en los dos subconjuntos correspondientes (entrenamiento y test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "manual-velvet",
   "metadata": {
    "id": "ZtW8KGwSS0eZ"
   },
   "outputs": [],
   "source": [
    "def part_traintest(X_traintest_cc, X_traintest_mlo, Y_traintest, rand_seed = 2021, frac_test = .2/.8):\n",
    "    \"\"\"\n",
    "    Function that makes a partition for training and testing from the original dataset\n",
    "        - X_traintest_cc is the array of CC-view images from the original dataset\n",
    "        - X_traintest_mlo is the array of MLO-view images from the original dataset\n",
    "        - Y_traintest is the array of labels from the original dataset\n",
    "        - rand_seed is a random seed number used during the sampling\n",
    "        - frac_test is the fraction of cases used in the test subset\n",
    "    Returns:\n",
    "        - X_train_cc is the train array of CC-images\n",
    "        - X_train_mlo is the train array of MLO-images\n",
    "        - Y_train is the train array of labels\n",
    "        - X_test_cc is the test array of CC-images\n",
    "        - X_test_mlo is the test array of MLO-images\n",
    "        - Y_test is the test array of labels\n",
    "    \"\"\"\n",
    "    np.random.seed(rand_seed)\n",
    "    index_test = np.array([], dtype = 'int64')\n",
    "    for i in np.unique(Y_traintest):\n",
    "        index_test = np.append(index_test, \n",
    "                               np.random.choice(list(np.where(Y_traintest == i)[0]), size = int(np.where(Y_traintest == i)[0].shape[0]*frac_test), replace = False))\n",
    "    X_train_cc = np.delete(X_traintest_cc, index_test, axis = 0)\n",
    "    X_train_mlo = np.delete(X_traintest_mlo, index_test, axis = 0)\n",
    "    Y_train = np.delete(Y_traintest, index_test)\n",
    "    X_test_cc = np.take(X_traintest_cc, index_test, axis = 0)\n",
    "    X_test_mlo = np.take(X_traintest_mlo, index_test, axis = 0)\n",
    "    Y_test = np.take(Y_traintest, index_test)\n",
    "    X_train_cc, X_train_mlo, Y_train = preprocess_data(X_train_cc, X_train_mlo, Y_train)\n",
    "    X_test_cc, X_test_mlo, Y_test = preprocess_data(X_test_cc, X_test_mlo, Y_test)\n",
    "\n",
    "    return X_train_cc, X_train_mlo, Y_train, X_test_cc, X_test_mlo, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-prayer",
   "metadata": {
    "id": "korean-press"
   },
   "source": [
    "Definimos los parámetros básicos para el proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "canadian-plant",
   "metadata": {
    "id": "incident-sender"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "no_epochs = 300\n",
    "rand_seed = 2021\n",
    "frac_test = .2/.8\n",
    "n_folds = 3\n",
    "learn_rate = [0.001]\n",
    "momentum = [0.5, 0.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-hindu",
   "metadata": {
    "id": "0j95AaVx5bCa"
   },
   "source": [
    "Dado el desbalance que sufren las categorías de la muestra de entrenamiento, forzamos el balanceo calculando las proporciones respecto a la clase más representada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tender-potential",
   "metadata": {
    "id": "pb5VnDmd5bCe"
   },
   "outputs": [],
   "source": [
    "label, counts = np.unique(Y_traintest, return_counts = True)\n",
    "class_weight = {}\n",
    "for lab, con in zip(label, counts):\n",
    "  class_weight.update({lab: round(max(counts)/con, 2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-popularity",
   "metadata": {
    "id": "GKOHQP5Q5e47"
   },
   "source": [
    "Definimos un callback para el entrenamiento de la red, de tal manera que nos aseguramos que el entrenamiento disminuye el learning rate cuando la pérdida sobre el conjutno de test ya no disminuye y detenemos el entrenamiento cuando dich pérdida tampoco disminuye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spoken-chamber",
   "metadata": {
    "id": "s_zS0yCB5gZ0"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience = 20, restore_best_weights = True)\n",
    "reduce_lr = ReduceLROnPlateau(factor = 0.5, patience = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-suggestion",
   "metadata": {
    "id": "yVvCVm45MlNh"
   },
   "source": [
    "Iteramos la definición y el entrenamiento de la red para no sesgar los resultados según el conjunto de entrenamiento y de test escogido en cada caso. Almacenamos el output de cada iteración para poder representarlos más adelante, evaluando cada red obtenida mediante el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sound-fleet",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1873748,
     "status": "ok",
     "timestamp": 1621185206275,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "studied-billy",
    "outputId": "3a45132c-2546-48c5-e821-a0f25f18011b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for combination 1/2 ...\n",
      "Learning rate = 0.001\n",
      "Momentum = 0.5\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1/3 ...\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/300\n",
      "10/10 [==============================] - 35s 1s/step - loss: 0.3628 - accuracy: 0.9693 - val_loss: 0.6256 - val_accuracy: 0.7500\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.2959 - accuracy: 0.9758 - val_loss: 0.6227 - val_accuracy: 0.8077\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.2712 - accuracy: 0.9897 - val_loss: 0.5814 - val_accuracy: 0.8077\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.2406 - accuracy: 0.9777 - val_loss: 0.5767 - val_accuracy: 0.8462\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.2185 - accuracy: 0.9982 - val_loss: 0.5805 - val_accuracy: 0.8077\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.2149 - accuracy: 0.9934 - val_loss: 0.5532 - val_accuracy: 0.8654\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1760 - accuracy: 1.0000 - val_loss: 0.5587 - val_accuracy: 0.8462\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.1743 - accuracy: 1.0000 - val_loss: 0.5470 - val_accuracy: 0.8462\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1591 - accuracy: 1.0000 - val_loss: 0.5480 - val_accuracy: 0.8462\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.1565 - accuracy: 1.0000 - val_loss: 0.5370 - val_accuracy: 0.8462\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1355 - accuracy: 1.0000 - val_loss: 0.5323 - val_accuracy: 0.8462\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.1459 - accuracy: 1.0000 - val_loss: 0.5275 - val_accuracy: 0.8462\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.1368 - accuracy: 1.0000 - val_loss: 0.5229 - val_accuracy: 0.8269\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1127 - accuracy: 1.0000 - val_loss: 0.5294 - val_accuracy: 0.8462\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.1256 - accuracy: 1.0000 - val_loss: 0.5171 - val_accuracy: 0.8462\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1110 - accuracy: 1.0000 - val_loss: 0.5195 - val_accuracy: 0.8462\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1053 - accuracy: 1.0000 - val_loss: 0.5152 - val_accuracy: 0.8462\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1068 - accuracy: 1.0000 - val_loss: 0.5149 - val_accuracy: 0.8462\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.1094 - accuracy: 1.0000 - val_loss: 0.5136 - val_accuracy: 0.8269\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1024 - accuracy: 1.0000 - val_loss: 0.5087 - val_accuracy: 0.8462\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0955 - accuracy: 1.0000 - val_loss: 0.5093 - val_accuracy: 0.8269\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1000 - accuracy: 1.0000 - val_loss: 0.5057 - val_accuracy: 0.8462\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0873 - accuracy: 1.0000 - val_loss: 0.5058 - val_accuracy: 0.8269\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0944 - accuracy: 1.0000 - val_loss: 0.5070 - val_accuracy: 0.8269\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0865 - accuracy: 1.0000 - val_loss: 0.5065 - val_accuracy: 0.8269\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0779 - accuracy: 1.0000 - val_loss: 0.5040 - val_accuracy: 0.8269\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.0787 - accuracy: 1.0000 - val_loss: 0.4994 - val_accuracy: 0.8462\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.0695 - accuracy: 1.0000 - val_loss: 0.5001 - val_accuracy: 0.8269\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0733 - accuracy: 1.0000 - val_loss: 0.4991 - val_accuracy: 0.8269\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.0707 - accuracy: 1.0000 - val_loss: 0.4978 - val_accuracy: 0.8269\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0726 - accuracy: 1.0000 - val_loss: 0.4961 - val_accuracy: 0.8269\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0784 - accuracy: 1.0000 - val_loss: 0.4962 - val_accuracy: 0.8269\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0702 - accuracy: 1.0000 - val_loss: 0.4950 - val_accuracy: 0.8269\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0651 - accuracy: 1.0000 - val_loss: 0.4941 - val_accuracy: 0.8269\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.0671 - accuracy: 1.0000 - val_loss: 0.4938 - val_accuracy: 0.8269\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0699 - accuracy: 1.0000 - val_loss: 0.4931 - val_accuracy: 0.8269\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.0667 - accuracy: 1.0000 - val_loss: 0.4934 - val_accuracy: 0.8269\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0616 - accuracy: 1.0000 - val_loss: 0.4913 - val_accuracy: 0.8269\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0641 - accuracy: 1.0000 - val_loss: 0.4910 - val_accuracy: 0.8269\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0622 - accuracy: 1.0000 - val_loss: 0.4924 - val_accuracy: 0.8269\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0599 - accuracy: 1.0000 - val_loss: 0.4904 - val_accuracy: 0.8269\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 0.0562 - accuracy: 1.0000 - val_loss: 0.4896 - val_accuracy: 0.8269\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0591 - accuracy: 1.0000 - val_loss: 0.4886 - val_accuracy: 0.8269\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 0.0523 - accuracy: 1.0000 - val_loss: 0.4896 - val_accuracy: 0.8269\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0516 - accuracy: 1.0000 - val_loss: 0.4896 - val_accuracy: 0.8269\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0603 - accuracy: 1.0000 - val_loss: 0.4881 - val_accuracy: 0.8269\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.0484 - accuracy: 1.0000 - val_loss: 0.4890 - val_accuracy: 0.8269\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0478 - accuracy: 1.0000 - val_loss: 0.4883 - val_accuracy: 0.8269\n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0516 - accuracy: 1.0000 - val_loss: 0.4882 - val_accuracy: 0.8269\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0490 - accuracy: 1.0000 - val_loss: 0.4889 - val_accuracy: 0.8077\n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.0494 - accuracy: 1.0000 - val_loss: 0.4892 - val_accuracy: 0.8077\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.0459 - accuracy: 1.0000 - val_loss: 0.4881 - val_accuracy: 0.8077\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.0441 - accuracy: 1.0000 - val_loss: 0.4876 - val_accuracy: 0.8269\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0488 - accuracy: 1.0000 - val_loss: 0.4874 - val_accuracy: 0.8269\n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 0.4871 - val_accuracy: 0.8077\n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0463 - accuracy: 1.0000 - val_loss: 0.4870 - val_accuracy: 0.8269\n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0502 - accuracy: 1.0000 - val_loss: 0.4868 - val_accuracy: 0.8269\n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.4868 - val_accuracy: 0.8269\n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0396 - accuracy: 1.0000 - val_loss: 0.4868 - val_accuracy: 0.8269\n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0456 - accuracy: 1.0000 - val_loss: 0.4864 - val_accuracy: 0.8269\n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0477 - accuracy: 1.0000 - val_loss: 0.4861 - val_accuracy: 0.8269\n",
      "Epoch 62/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0474 - accuracy: 1.0000 - val_loss: 0.4862 - val_accuracy: 0.8269\n",
      "Epoch 63/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 0.4863 - val_accuracy: 0.8269\n",
      "Epoch 64/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0488 - accuracy: 1.0000 - val_loss: 0.4860 - val_accuracy: 0.8269\n",
      "Epoch 65/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.4861 - val_accuracy: 0.8269\n",
      "Epoch 66/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.0457 - accuracy: 1.0000 - val_loss: 0.4862 - val_accuracy: 0.8269\n",
      "Epoch 67/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0393 - accuracy: 1.0000 - val_loss: 0.4856 - val_accuracy: 0.8269\n",
      "Epoch 68/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0423 - accuracy: 1.0000 - val_loss: 0.4862 - val_accuracy: 0.8077\n",
      "Epoch 69/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 0.4859 - val_accuracy: 0.8269\n",
      "Epoch 70/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0415 - accuracy: 1.0000 - val_loss: 0.4859 - val_accuracy: 0.8269\n",
      "Epoch 71/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0402 - accuracy: 1.0000 - val_loss: 0.4858 - val_accuracy: 0.8269\n",
      "Epoch 72/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0411 - accuracy: 1.0000 - val_loss: 0.4854 - val_accuracy: 0.8269\n",
      "Epoch 73/300\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.4858 - val_accuracy: 0.8269\n",
      "Epoch 74/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 0.4853 - val_accuracy: 0.8269\n",
      "Epoch 75/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.4857 - val_accuracy: 0.8077\n",
      "Epoch 76/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.4852 - val_accuracy: 0.8269\n",
      "Epoch 77/300\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.0419 - accuracy: 1.0000 - val_loss: 0.4852 - val_accuracy: 0.8269\n",
      "Epoch 78/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.4850 - val_accuracy: 0.8269\n",
      "Epoch 79/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.4849 - val_accuracy: 0.8269\n",
      "Epoch 80/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0396 - accuracy: 1.0000 - val_loss: 0.4853 - val_accuracy: 0.8077\n",
      "Epoch 81/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0427 - accuracy: 1.0000 - val_loss: 0.4850 - val_accuracy: 0.8269\n",
      "Epoch 82/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.4850 - val_accuracy: 0.8269\n",
      "Epoch 83/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 0.4851 - val_accuracy: 0.8077\n",
      "Epoch 84/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.4845 - val_accuracy: 0.8269\n",
      "Epoch 85/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.4848 - val_accuracy: 0.8269\n",
      "Epoch 86/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.8269\n",
      "Epoch 87/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.8269\n",
      "Epoch 88/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0412 - accuracy: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.8269\n",
      "Epoch 89/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.8269\n",
      "Epoch 90/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0343 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.8269\n",
      "Epoch 91/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0393 - accuracy: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.8269\n",
      "Epoch 92/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.8269\n",
      "Epoch 93/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.8269\n",
      "Epoch 94/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.8269\n",
      "Epoch 95/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0377 - accuracy: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.8269\n",
      "Epoch 96/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0355 - accuracy: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.8269\n",
      "Epoch 97/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0400 - accuracy: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.8269\n",
      "Epoch 98/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.8269\n",
      "Epoch 99/300\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.8269\n",
      "Epoch 100/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0394 - accuracy: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.8269\n",
      "Epoch 101/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.8269\n",
      "Epoch 102/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.8269\n",
      "Epoch 103/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.8269\n",
      "Epoch 104/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.8269\n",
      "------------------------------------------------------------------------\n",
      "Score for fold 1: loss of 0.6; accuracy of 76.92%\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2/3 ...\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/300\n",
      "10/10 [==============================] - 23s 906ms/step - loss: 0.5159 - accuracy: 0.9431 - val_loss: 0.3167 - val_accuracy: 0.9231\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.3881 - accuracy: 0.9594 - val_loss: 0.3460 - val_accuracy: 0.9423\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.4169 - accuracy: 0.9347 - val_loss: 0.2949 - val_accuracy: 0.9231\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.3313 - accuracy: 0.9793 - val_loss: 0.2862 - val_accuracy: 0.9423\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.3521 - accuracy: 0.9614 - val_loss: 0.2714 - val_accuracy: 0.9231\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.3499 - accuracy: 0.9861 - val_loss: 0.2646 - val_accuracy: 0.9423\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.3419 - accuracy: 0.9544 - val_loss: 0.2587 - val_accuracy: 0.9231\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.3342 - accuracy: 0.9741 - val_loss: 0.2489 - val_accuracy: 0.9423\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.3195 - accuracy: 0.9492 - val_loss: 0.2468 - val_accuracy: 0.9423\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.2756 - accuracy: 0.9824 - val_loss: 0.2472 - val_accuracy: 0.9423\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.2912 - accuracy: 0.9748 - val_loss: 0.2393 - val_accuracy: 0.9231\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.2421 - accuracy: 0.9800 - val_loss: 0.2357 - val_accuracy: 0.9423\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.2400 - accuracy: 0.9849 - val_loss: 0.2320 - val_accuracy: 0.9231\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.2697 - accuracy: 0.9805 - val_loss: 0.2326 - val_accuracy: 0.9231\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.2086 - accuracy: 0.9846 - val_loss: 0.2298 - val_accuracy: 0.9423\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.2015 - accuracy: 0.9938 - val_loss: 0.2259 - val_accuracy: 0.9231\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.2429 - accuracy: 0.9867 - val_loss: 0.2248 - val_accuracy: 0.9231\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.2400 - accuracy: 0.9817 - val_loss: 0.2233 - val_accuracy: 0.9231\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.1971 - accuracy: 0.9774 - val_loss: 0.2224 - val_accuracy: 0.9423\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1877 - accuracy: 0.9903 - val_loss: 0.2216 - val_accuracy: 0.9231\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.1866 - accuracy: 0.9877 - val_loss: 0.2194 - val_accuracy: 0.9231\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.1828 - accuracy: 0.9896 - val_loss: 0.2187 - val_accuracy: 0.9231\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.2138 - accuracy: 0.9602 - val_loss: 0.2173 - val_accuracy: 0.9231\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.1367 - accuracy: 0.9923 - val_loss: 0.2163 - val_accuracy: 0.9231\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.1832 - accuracy: 0.9810 - val_loss: 0.2157 - val_accuracy: 0.9231\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.1923 - accuracy: 0.9810 - val_loss: 0.2156 - val_accuracy: 0.9231\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1602 - accuracy: 0.9847 - val_loss: 0.2147 - val_accuracy: 0.9231\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1532 - accuracy: 0.9834 - val_loss: 0.2144 - val_accuracy: 0.9231\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.1390 - accuracy: 0.9890 - val_loss: 0.2138 - val_accuracy: 0.9231\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1282 - accuracy: 0.9925 - val_loss: 0.2137 - val_accuracy: 0.9231\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1446 - accuracy: 0.9872 - val_loss: 0.2128 - val_accuracy: 0.9231\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1926 - accuracy: 0.9785 - val_loss: 0.2171 - val_accuracy: 0.9231\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.1277 - accuracy: 0.9940 - val_loss: 0.2121 - val_accuracy: 0.9231\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.1553 - accuracy: 0.9783 - val_loss: 0.2124 - val_accuracy: 0.9231\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.1520 - accuracy: 0.9785 - val_loss: 0.2123 - val_accuracy: 0.9231\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1284 - accuracy: 0.9895 - val_loss: 0.2129 - val_accuracy: 0.9231\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.1290 - accuracy: 0.9860 - val_loss: 0.2116 - val_accuracy: 0.9231\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1276 - accuracy: 0.9907 - val_loss: 0.2119 - val_accuracy: 0.9231\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1429 - accuracy: 0.9785 - val_loss: 0.2118 - val_accuracy: 0.9231\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1247 - accuracy: 0.9889 - val_loss: 0.2115 - val_accuracy: 0.9231\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.1314 - accuracy: 0.9949 - val_loss: 0.2114 - val_accuracy: 0.9231\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 0.1502 - accuracy: 0.9816 - val_loss: 0.2112 - val_accuracy: 0.9231\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1025 - accuracy: 0.9940 - val_loss: 0.2111 - val_accuracy: 0.9423\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1179 - accuracy: 0.9842 - val_loss: 0.2111 - val_accuracy: 0.9231\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.1115 - accuracy: 0.9915 - val_loss: 0.2099 - val_accuracy: 0.9231\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.1116 - accuracy: 0.9892 - val_loss: 0.2111 - val_accuracy: 0.9231\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1441 - accuracy: 0.9810 - val_loss: 0.2136 - val_accuracy: 0.9231\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1259 - accuracy: 0.9816 - val_loss: 0.2114 - val_accuracy: 0.9231\n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1223 - accuracy: 0.9899 - val_loss: 0.2115 - val_accuracy: 0.9231\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1078 - accuracy: 0.9810 - val_loss: 0.2105 - val_accuracy: 0.9231\n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 4s 439ms/step - loss: 0.1029 - accuracy: 0.9949 - val_loss: 0.2107 - val_accuracy: 0.9231\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 0.0926 - accuracy: 0.9924 - val_loss: 0.2109 - val_accuracy: 0.9231\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.1173 - accuracy: 0.9810 - val_loss: 0.2107 - val_accuracy: 0.9231\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.1160 - accuracy: 0.9798 - val_loss: 0.2113 - val_accuracy: 0.9231\n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.1019 - accuracy: 0.9888 - val_loss: 0.2117 - val_accuracy: 0.9231\n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0929 - accuracy: 0.9935 - val_loss: 0.2118 - val_accuracy: 0.9231\n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.1019 - accuracy: 0.9842 - val_loss: 0.2116 - val_accuracy: 0.9231\n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.1160 - accuracy: 0.9785 - val_loss: 0.2115 - val_accuracy: 0.9231\n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.1058 - accuracy: 0.9860 - val_loss: 0.2115 - val_accuracy: 0.9231\n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1078 - accuracy: 0.9760 - val_loss: 0.2115 - val_accuracy: 0.9231\n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.1025 - accuracy: 0.9921 - val_loss: 0.2114 - val_accuracy: 0.9231\n",
      "Epoch 62/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.1070 - accuracy: 0.9878 - val_loss: 0.2114 - val_accuracy: 0.9231\n",
      "Epoch 63/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0935 - accuracy: 0.9940 - val_loss: 0.2115 - val_accuracy: 0.9231\n",
      "Epoch 64/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0846 - accuracy: 0.9935 - val_loss: 0.2117 - val_accuracy: 0.9231\n",
      "Epoch 65/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.1073 - accuracy: 0.9888 - val_loss: 0.2114 - val_accuracy: 0.9231\n",
      "------------------------------------------------------------------------\n",
      "Score for fold 2: loss of 0.62; accuracy of 78.85%\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3/3 ...\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/300\n",
      "10/10 [==============================] - 22s 900ms/step - loss: 0.4599 - accuracy: 0.9094 - val_loss: 0.3581 - val_accuracy: 0.9423\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.4083 - accuracy: 0.8981 - val_loss: 0.3628 - val_accuracy: 0.9231\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.3793 - accuracy: 0.9482 - val_loss: 0.3278 - val_accuracy: 0.9231\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.3960 - accuracy: 0.9374 - val_loss: 0.3379 - val_accuracy: 0.9231\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.3101 - accuracy: 0.9514 - val_loss: 0.3250 - val_accuracy: 0.9423\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.2992 - accuracy: 0.9683 - val_loss: 0.3328 - val_accuracy: 0.9038\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.2620 - accuracy: 0.9576 - val_loss: 0.3118 - val_accuracy: 0.9423\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.2579 - accuracy: 0.9525 - val_loss: 0.3088 - val_accuracy: 0.9231\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.3149 - accuracy: 0.9380 - val_loss: 0.2990 - val_accuracy: 0.9423\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.3032 - accuracy: 0.9435 - val_loss: 0.2929 - val_accuracy: 0.9231\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.2113 - accuracy: 0.9907 - val_loss: 0.3012 - val_accuracy: 0.9423\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.2710 - accuracy: 0.9638 - val_loss: 0.2827 - val_accuracy: 0.9423\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.2041 - accuracy: 0.9774 - val_loss: 0.2827 - val_accuracy: 0.9231\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1741 - accuracy: 0.9932 - val_loss: 0.2883 - val_accuracy: 0.9423\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.2109 - accuracy: 0.9543 - val_loss: 0.2795 - val_accuracy: 0.9231\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.1950 - accuracy: 0.9709 - val_loss: 0.2761 - val_accuracy: 0.9038\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 4s 421ms/step - loss: 0.1869 - accuracy: 0.9698 - val_loss: 0.2788 - val_accuracy: 0.9231\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.2007 - accuracy: 0.9729 - val_loss: 0.2771 - val_accuracy: 0.9231\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.1586 - accuracy: 0.9769 - val_loss: 0.2759 - val_accuracy: 0.9231\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1830 - accuracy: 0.9749 - val_loss: 0.2831 - val_accuracy: 0.9423\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.1444 - accuracy: 0.9882 - val_loss: 0.2767 - val_accuracy: 0.9231\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.1590 - accuracy: 0.9900 - val_loss: 0.2772 - val_accuracy: 0.9231\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.1861 - accuracy: 0.9751 - val_loss: 0.2720 - val_accuracy: 0.9038\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.1688 - accuracy: 0.9760 - val_loss: 0.2733 - val_accuracy: 0.9038\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.1630 - accuracy: 0.9777 - val_loss: 0.2739 - val_accuracy: 0.9038\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.1428 - accuracy: 0.9949 - val_loss: 0.2829 - val_accuracy: 0.9423\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.1458 - accuracy: 0.9907 - val_loss: 0.2715 - val_accuracy: 0.9038\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.1540 - accuracy: 0.9741 - val_loss: 0.2717 - val_accuracy: 0.9038\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.1451 - accuracy: 0.9913 - val_loss: 0.2727 - val_accuracy: 0.9038\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.1463 - accuracy: 0.9949 - val_loss: 0.2750 - val_accuracy: 0.9231\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.1117 - accuracy: 0.9892 - val_loss: 0.2774 - val_accuracy: 0.9231\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.1384 - accuracy: 0.9885 - val_loss: 0.2699 - val_accuracy: 0.9038\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.1354 - accuracy: 0.9867 - val_loss: 0.2734 - val_accuracy: 0.9038\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.1417 - accuracy: 0.9828 - val_loss: 0.2673 - val_accuracy: 0.9038\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1466 - accuracy: 0.9769 - val_loss: 0.2675 - val_accuracy: 0.9038\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0976 - accuracy: 0.9977 - val_loss: 0.2700 - val_accuracy: 0.9038\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.1116 - accuracy: 0.9988 - val_loss: 0.2695 - val_accuracy: 0.9038\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.1031 - accuracy: 0.9975 - val_loss: 0.2719 - val_accuracy: 0.9038\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.1087 - accuracy: 0.9957 - val_loss: 0.2714 - val_accuracy: 0.9038\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1042 - accuracy: 0.9982 - val_loss: 0.2721 - val_accuracy: 0.9038\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1047 - accuracy: 0.9946 - val_loss: 0.2728 - val_accuracy: 0.9038\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0974 - accuracy: 0.9967 - val_loss: 0.2724 - val_accuracy: 0.9038\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0943 - accuracy: 0.9957 - val_loss: 0.2728 - val_accuracy: 0.9038\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.1042 - accuracy: 0.9957 - val_loss: 0.2733 - val_accuracy: 0.9038\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.1379 - accuracy: 0.9828 - val_loss: 0.2729 - val_accuracy: 0.9038\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.1201 - accuracy: 0.9828 - val_loss: 0.2723 - val_accuracy: 0.9038\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.1045 - accuracy: 0.9946 - val_loss: 0.2721 - val_accuracy: 0.9038\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0962 - accuracy: 0.9975 - val_loss: 0.2718 - val_accuracy: 0.9038\n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.1109 - accuracy: 0.9967 - val_loss: 0.2721 - val_accuracy: 0.9038\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0961 - accuracy: 0.9946 - val_loss: 0.2718 - val_accuracy: 0.9038\n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.1002 - accuracy: 0.9988 - val_loss: 0.2716 - val_accuracy: 0.9038\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0993 - accuracy: 0.9967 - val_loss: 0.2723 - val_accuracy: 0.9038\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0914 - accuracy: 0.9967 - val_loss: 0.2722 - val_accuracy: 0.9038\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0883 - accuracy: 0.9975 - val_loss: 0.2722 - val_accuracy: 0.9038\n",
      "------------------------------------------------------------------------\n",
      "Score for fold 3: loss of 0.61; accuracy of 76.92%\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6 - Accuracy: 0.77%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.62 - Accuracy: 0.79%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.61 - Accuracy: 0.77%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds (LR = 0.001, mtm = 0.5):\n",
      "> Accuracy: 0.78 (+- 0.01)\n",
      "> Loss: 0.61 (+- 0.01)\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Training for combination 2/2 ...\n",
      "Learning rate = 0.001\n",
      "Momentum = 0.8\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1/3 ...\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/300\n",
      "10/10 [==============================] - 25s 935ms/step - loss: 0.3280 - accuracy: 0.9677 - val_loss: 0.6051 - val_accuracy: 0.7885\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.2808 - accuracy: 0.9701 - val_loss: 0.6646 - val_accuracy: 0.7885\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.2365 - accuracy: 0.9828 - val_loss: 0.5663 - val_accuracy: 0.8077\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.1661 - accuracy: 0.9913 - val_loss: 0.5317 - val_accuracy: 0.8462\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.1329 - accuracy: 1.0000 - val_loss: 0.5295 - val_accuracy: 0.8462\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.1165 - accuracy: 1.0000 - val_loss: 0.5228 - val_accuracy: 0.8462\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.1118 - accuracy: 1.0000 - val_loss: 0.5109 - val_accuracy: 0.8462\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.1053 - accuracy: 1.0000 - val_loss: 0.5098 - val_accuracy: 0.8077\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0999 - accuracy: 1.0000 - val_loss: 0.5024 - val_accuracy: 0.7885\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0902 - accuracy: 1.0000 - val_loss: 0.5000 - val_accuracy: 0.8269\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0809 - accuracy: 1.0000 - val_loss: 0.5034 - val_accuracy: 0.8077\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.0761 - accuracy: 1.0000 - val_loss: 0.4943 - val_accuracy: 0.8269\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.0694 - accuracy: 1.0000 - val_loss: 0.4989 - val_accuracy: 0.8077\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.0698 - accuracy: 1.0000 - val_loss: 0.4912 - val_accuracy: 0.8077\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0604 - accuracy: 1.0000 - val_loss: 0.4911 - val_accuracy: 0.8269\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0631 - accuracy: 1.0000 - val_loss: 0.4892 - val_accuracy: 0.8269\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0598 - accuracy: 1.0000 - val_loss: 0.4905 - val_accuracy: 0.8077\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0604 - accuracy: 1.0000 - val_loss: 0.4890 - val_accuracy: 0.8077\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0507 - accuracy: 1.0000 - val_loss: 0.4883 - val_accuracy: 0.8077\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 0.4885 - val_accuracy: 0.8269\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 0.4904 - val_accuracy: 0.8077\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.0430 - accuracy: 1.0000 - val_loss: 0.4870 - val_accuracy: 0.8077\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.8269\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.4864 - val_accuracy: 0.8269\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 0.4854 - val_accuracy: 0.8269\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.0438 - accuracy: 1.0000 - val_loss: 0.4855 - val_accuracy: 0.8077\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.4834 - val_accuracy: 0.8077\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0426 - accuracy: 1.0000 - val_loss: 0.4839 - val_accuracy: 0.8269\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.4851 - val_accuracy: 0.8077\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.4837 - val_accuracy: 0.8269\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.4843 - val_accuracy: 0.8077\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.8077\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.0377 - accuracy: 1.0000 - val_loss: 0.4849 - val_accuracy: 0.8077\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 0.4832 - val_accuracy: 0.8077\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.4839 - val_accuracy: 0.8077\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0298 - accuracy: 1.0000 - val_loss: 0.4833 - val_accuracy: 0.8077\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.4831 - val_accuracy: 0.8269\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.4844 - val_accuracy: 0.8077\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0324 - accuracy: 1.0000 - val_loss: 0.4852 - val_accuracy: 0.8077\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0307 - accuracy: 1.0000 - val_loss: 0.4839 - val_accuracy: 0.8077\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 0.4829 - val_accuracy: 0.8269\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.4822 - val_accuracy: 0.8269\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.4836 - val_accuracy: 0.8077\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 4s 437ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.4836 - val_accuracy: 0.8077\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.4838 - val_accuracy: 0.8077\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.4836 - val_accuracy: 0.8077\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.4832 - val_accuracy: 0.8269\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.4833 - val_accuracy: 0.8077\n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.4834 - val_accuracy: 0.8077\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.4836 - val_accuracy: 0.8077\n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.4837 - val_accuracy: 0.8077\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.0272 - accuracy: 1.0000 - val_loss: 0.4837 - val_accuracy: 0.8077\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.4838 - val_accuracy: 0.8077\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 0.4839 - val_accuracy: 0.8077\n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.4837 - val_accuracy: 0.8077\n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 4s 438ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.4836 - val_accuracy: 0.8077\n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.4836 - val_accuracy: 0.8077\n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.4837 - val_accuracy: 0.8077\n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.4836 - val_accuracy: 0.8077\n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.4836 - val_accuracy: 0.8077\n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 4s 437ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 0.4836 - val_accuracy: 0.8077\n",
      "Epoch 62/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.4835 - val_accuracy: 0.8077\n",
      "------------------------------------------------------------------------\n",
      "Score for fold 1: loss of 0.61; accuracy of 76.92%\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2/3 ...\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/300\n",
      "10/10 [==============================] - 22s 915ms/step - loss: 0.5677 - accuracy: 0.9034 - val_loss: 0.3257 - val_accuracy: 0.9423\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.4786 - accuracy: 0.9134 - val_loss: 0.2864 - val_accuracy: 0.9231\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.4114 - accuracy: 0.9261 - val_loss: 0.2665 - val_accuracy: 0.9231\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.3096 - accuracy: 0.9801 - val_loss: 0.2430 - val_accuracy: 0.9423\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.2248 - accuracy: 0.9833 - val_loss: 0.2423 - val_accuracy: 0.9423\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.2660 - accuracy: 0.9579 - val_loss: 0.2274 - val_accuracy: 0.9423\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.2187 - accuracy: 0.9679 - val_loss: 0.2268 - val_accuracy: 0.9423\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 4s 437ms/step - loss: 0.2019 - accuracy: 0.9727 - val_loss: 0.2203 - val_accuracy: 0.9231\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.1797 - accuracy: 0.9922 - val_loss: 0.2168 - val_accuracy: 0.9231\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.1725 - accuracy: 0.9882 - val_loss: 0.2161 - val_accuracy: 0.9231\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.1410 - accuracy: 0.9889 - val_loss: 0.2136 - val_accuracy: 0.9231\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.1678 - accuracy: 0.9729 - val_loss: 0.2129 - val_accuracy: 0.9231\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.1222 - accuracy: 0.9910 - val_loss: 0.2127 - val_accuracy: 0.9231\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.1577 - accuracy: 0.9788 - val_loss: 0.2115 - val_accuracy: 0.9231\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.1282 - accuracy: 0.9899 - val_loss: 0.2120 - val_accuracy: 0.9231\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 4s 439ms/step - loss: 0.1292 - accuracy: 0.9949 - val_loss: 0.2143 - val_accuracy: 0.9231\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0990 - accuracy: 0.9940 - val_loss: 0.2112 - val_accuracy: 0.9231\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.1145 - accuracy: 0.9949 - val_loss: 0.2109 - val_accuracy: 0.9231\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.0967 - accuracy: 0.9964 - val_loss: 0.2109 - val_accuracy: 0.9231\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 4s 438ms/step - loss: 0.1147 - accuracy: 0.9895 - val_loss: 0.2099 - val_accuracy: 0.9231\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.1267 - accuracy: 0.9842 - val_loss: 0.2131 - val_accuracy: 0.9231\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.1244 - accuracy: 0.9935 - val_loss: 0.2089 - val_accuracy: 0.9231\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.1063 - accuracy: 0.9845 - val_loss: 0.2142 - val_accuracy: 0.9231\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.1138 - accuracy: 0.9901 - val_loss: 0.2119 - val_accuracy: 0.9231\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0988 - accuracy: 0.9816 - val_loss: 0.2145 - val_accuracy: 0.9231\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.0919 - accuracy: 0.9826 - val_loss: 0.2119 - val_accuracy: 0.9231\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.0818 - accuracy: 0.9880 - val_loss: 0.2116 - val_accuracy: 0.9231\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0817 - accuracy: 0.9880 - val_loss: 0.2107 - val_accuracy: 0.9231\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.0805 - accuracy: 0.9924 - val_loss: 0.2145 - val_accuracy: 0.9231\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.0826 - accuracy: 0.9892 - val_loss: 0.2140 - val_accuracy: 0.9231\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.0696 - accuracy: 0.9892 - val_loss: 0.2149 - val_accuracy: 0.9231\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0873 - accuracy: 0.9867 - val_loss: 0.2136 - val_accuracy: 0.9231\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.1001 - accuracy: 0.9741 - val_loss: 0.2137 - val_accuracy: 0.9231\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.0808 - accuracy: 0.9942 - val_loss: 0.2139 - val_accuracy: 0.9231\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0675 - accuracy: 0.9957 - val_loss: 0.2154 - val_accuracy: 0.9231\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 4s 437ms/step - loss: 0.1116 - accuracy: 0.9656 - val_loss: 0.2149 - val_accuracy: 0.9231\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.0689 - accuracy: 0.9949 - val_loss: 0.2152 - val_accuracy: 0.9231\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.0746 - accuracy: 0.9901 - val_loss: 0.2155 - val_accuracy: 0.9231\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.0631 - accuracy: 0.9964 - val_loss: 0.2152 - val_accuracy: 0.9231\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0643 - accuracy: 0.9935 - val_loss: 0.2146 - val_accuracy: 0.9231\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0838 - accuracy: 0.9852 - val_loss: 0.2161 - val_accuracy: 0.9231\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 0.1136 - accuracy: 0.9656 - val_loss: 0.2153 - val_accuracy: 0.9231\n",
      "------------------------------------------------------------------------\n",
      "Score for fold 2: loss of 0.63; accuracy of 78.85%\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3/3 ...\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/300\n",
      "10/10 [==============================] - 22s 913ms/step - loss: 0.5089 - accuracy: 0.8674 - val_loss: 0.3513 - val_accuracy: 0.9038\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.3775 - accuracy: 0.9353 - val_loss: 0.3189 - val_accuracy: 0.9231\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.2719 - accuracy: 0.9745 - val_loss: 0.3409 - val_accuracy: 0.9038\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 4s 441ms/step - loss: 0.2768 - accuracy: 0.9330 - val_loss: 0.3217 - val_accuracy: 0.9231\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.2325 - accuracy: 0.9838 - val_loss: 0.2873 - val_accuracy: 0.9231\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.2461 - accuracy: 0.9589 - val_loss: 0.2792 - val_accuracy: 0.9231\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.1938 - accuracy: 0.9614 - val_loss: 0.2798 - val_accuracy: 0.9231\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.2102 - accuracy: 0.9647 - val_loss: 0.2704 - val_accuracy: 0.9231\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.1574 - accuracy: 0.9887 - val_loss: 0.2868 - val_accuracy: 0.9423\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.1381 - accuracy: 0.9855 - val_loss: 0.2741 - val_accuracy: 0.9231\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.1395 - accuracy: 0.9920 - val_loss: 0.2699 - val_accuracy: 0.9038\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.1307 - accuracy: 0.9946 - val_loss: 0.2740 - val_accuracy: 0.9231\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.1440 - accuracy: 0.9864 - val_loss: 0.2786 - val_accuracy: 0.9231\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.1286 - accuracy: 0.9932 - val_loss: 0.2676 - val_accuracy: 0.9038\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 4s 438ms/step - loss: 0.1393 - accuracy: 0.9860 - val_loss: 0.2748 - val_accuracy: 0.9038\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 4s 438ms/step - loss: 0.1220 - accuracy: 0.9913 - val_loss: 0.2665 - val_accuracy: 0.9038\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.0888 - accuracy: 0.9975 - val_loss: 0.2745 - val_accuracy: 0.9038\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.0995 - accuracy: 0.9982 - val_loss: 0.2727 - val_accuracy: 0.9038\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.0927 - accuracy: 0.9957 - val_loss: 0.2735 - val_accuracy: 0.9038\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.1087 - accuracy: 0.9946 - val_loss: 0.2722 - val_accuracy: 0.9038\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 4s 437ms/step - loss: 0.1028 - accuracy: 0.9885 - val_loss: 0.2686 - val_accuracy: 0.9038\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.0831 - accuracy: 0.9967 - val_loss: 0.2720 - val_accuracy: 0.9038\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 4s 437ms/step - loss: 0.0789 - accuracy: 0.9967 - val_loss: 0.2763 - val_accuracy: 0.9038\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 0.0795 - accuracy: 0.9967 - val_loss: 0.2770 - val_accuracy: 0.9038\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0845 - accuracy: 0.9988 - val_loss: 0.2696 - val_accuracy: 0.9038\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 4s 437ms/step - loss: 0.0763 - accuracy: 0.9957 - val_loss: 0.2775 - val_accuracy: 0.9038\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0779 - accuracy: 0.9975 - val_loss: 0.2723 - val_accuracy: 0.9038\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.0618 - accuracy: 0.9957 - val_loss: 0.2729 - val_accuracy: 0.9038\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.0779 - accuracy: 0.9932 - val_loss: 0.2737 - val_accuracy: 0.9038\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 4s 435ms/step - loss: 0.0714 - accuracy: 0.9982 - val_loss: 0.2724 - val_accuracy: 0.9038\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.1004 - accuracy: 0.9828 - val_loss: 0.2736 - val_accuracy: 0.9038\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.0796 - accuracy: 0.9913 - val_loss: 0.2739 - val_accuracy: 0.9038\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.0678 - accuracy: 0.9982 - val_loss: 0.2734 - val_accuracy: 0.9038\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0866 - accuracy: 0.9932 - val_loss: 0.2734 - val_accuracy: 0.9038\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0813 - accuracy: 0.9885 - val_loss: 0.2738 - val_accuracy: 0.9038\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.0806 - accuracy: 0.9967 - val_loss: 0.2731 - val_accuracy: 0.9038\n",
      "------------------------------------------------------------------------\n",
      "Score for fold 3: loss of 0.62; accuracy of 75.0%\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.61 - Accuracy: 0.77%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.63 - Accuracy: 0.79%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.62 - Accuracy: 0.75%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds (LR = 0.001, mtm = 0.8):\n",
      "> Accuracy: 0.77 (+- 0.02)\n",
      "> Loss: 0.62 (+- 0.01)\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tot_comb = len(learn_rate) * len(momentum)\n",
    "glob_param = np.empty([tot_comb, 2])\n",
    "\n",
    "history_list = []\n",
    "scores_glob_array = np.empty([tot_comb, n_folds, 2])\n",
    "\n",
    "for idx, x in enumerate(itertools.product(learn_rate, momentum)):\n",
    "    learn_rate = x[0]\n",
    "    momentum = x[1]\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for combination {idx + 1}/{tot_comb} ...')\n",
    "    print(f'Learning rate = {learn_rate}')\n",
    "    print(f'Momentum = {momentum}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "    history_array = np.array([])\n",
    "    scores_array = np.empty([n_folds, 2])\n",
    "\n",
    "    glob_param[idx, 0] = learn_rate\n",
    "    glob_param[idx, 1] = momentum\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        # Generate a print\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold + 1}/{n_folds} ...')\n",
    "        print('------------------------------------------------------------------------')\n",
    "\n",
    "        # Generate the fold sample for train-test\n",
    "        X_train_cc, X_train_mlo, Y_train, X_test_cc, X_test_mlo, Y_test = part_traintest(X_traintest_cc, X_traintest_mlo, Y_traintest, rand_seed + fold, frac_test)\n",
    "        \n",
    "        # Define the model architecture\n",
    "        model_2ramas = DenseNet_2Ramas(model_cc, model_mlo, rand_seed, learn_rate, momentum)\n",
    "        \n",
    "        # Fit data to model\n",
    "        history = model_2ramas.fit([X_train_cc, X_train_mlo], Y_train,\n",
    "                                  batch_size = batch_size,\n",
    "                                  epochs = no_epochs,\n",
    "                                  validation_data = ([X_test_cc, X_test_mlo], Y_test),\n",
    "                                  class_weight = class_weight,\n",
    "                                  verbose = 1,\n",
    "                                  callbacks = [early_stopping, reduce_lr])\n",
    "\n",
    "        # Generate generalization metrics\n",
    "        scores_array[fold, :] = model_2ramas.evaluate([X_val_cc, X_val_mlo], Y_val, verbose = 0)\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Score for fold {fold + 1}: {model_2ramas.metrics_names[0]} of {round(scores_array[fold, 0], 2)}; {model_2ramas.metrics_names[1]} of {round(scores_array[fold, 1]*100, 2)}%')\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print('')\n",
    "        \n",
    "        # Append history callback into array\n",
    "        history_array = np.append(history_array, [history])\n",
    "        \n",
    "    # == Provide average scores ==\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score per fold')\n",
    "    for i in range(0, scores_array.shape[0]):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Fold {i + 1} - Loss: {round(scores_array[i, 0], 2)} - Accuracy: {round(scores_array[i, 1], 2)}%')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Average scores for all folds (LR = {learn_rate}, mtm = {momentum}):')\n",
    "    print(f'> Accuracy: {round(np.mean(scores_array[:, 1]), 2)} (+- {round(np.std(scores_array[:, 1]), 2)})')\n",
    "    print(f'> Loss: {round(np.mean(scores_array[:, 0]), 2)} (+- {round(np.std(scores_array[:, 0]), 2)})')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('')\n",
    "    print('')\n",
    "\n",
    "    idx_best_hist = np.argmax(scores_array[:, 1])\n",
    "    history_list.append(history_array[idx_best_hist])\n",
    "    \n",
    "    scores_glob_array[idx, :, :] = scores_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-education",
   "metadata": {
    "id": "super-progressive"
   },
   "source": [
    "## Representación de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-medicine",
   "metadata": {
    "id": "streaming-recruitment"
   },
   "source": [
    "Definimos una serie de funciones auxiliares para facilitar la visualización de resultados (representación de la evolución de las métricas durante el proceso de entrenamiento y resultados de la matriz de confusión finalmente obtenida)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "strategic-lindsay",
   "metadata": {
    "id": "published-lingerie"
   },
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    \"\"\"\n",
    "    Function that plots the metrics from the training process\n",
    "        - history_array is the array of histories generated during the training process\n",
    "        - no_epochs is number of epochs fixed for the training process\n",
    "    Returns:\n",
    "        - accuracy is the output array of accuracies\n",
    "        - val_accuracy is the output array of validation accuracies\n",
    "        - loss is the output array of losses\n",
    "        - val_loss is the output array of validation losses\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize = (15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc = 'upper left')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc = 'upper right')\n",
    "    \n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"\n",
    "    Credit to:\n",
    "    https://gist.github.com/zachguo/10296432\n",
    "    \n",
    "    Function that makes a pretty print for confusion matrixes\n",
    "        - cm is the array of histories generated during the training process\n",
    "        - labels is number of epochs fixed for the training process\n",
    "        - hide_zeroes is a boolean that allows the user to hide the zeroes in the matrix\n",
    "        - hide_diagonal is a boolean that allows the user to hide the diagonal in the matrix\n",
    "        - hide_threshold is a number that allows the user to hide results in the matrix below that value\n",
    "    \"\"\"    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-breath",
   "metadata": {
    "id": "n5Z7WzPF1uss"
   },
   "source": [
    "Mostramos por pantalla los resultados medios de todas las combinaciones de parámetros y elegimos el modelo que mejores resultados ofrece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "homeless-speed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1874600,
     "status": "ok",
     "timestamp": 1621185207140,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "LNmPI3ko1ukf",
    "outputId": "1950dbd4-f9c6-4e6c-ad61-588cd276f1d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados promedios del entrenamiento DenseNet-CC+MLO:\n",
      "- LR=0.001 / mom=0.5:\tAcc=0.78 (+- 0.01) - Loss=0.61 (+- 0.01)\n",
      "- LR=0.001 / mom=0.8:\tAcc=0.77 (+- 0.02) - Loss=0.62 (+- 0.01)\n",
      "\n",
      "El mejor resultado para la DenseNet-CC+MLO se obtiene para LR = 0.001 y momentum = 0.5.\n"
     ]
    }
   ],
   "source": [
    "print(f'Resultados promedios del entrenamiento DenseNet-CC+MLO:')\n",
    "for i in range(len(glob_param)):\n",
    "    print(f'- LR={glob_param[i, 0]} / mom={glob_param[i, 1]}:\\tAcc={round(np.mean(scores_glob_array[i, :, 1]), 2)} (+- {round(np.std(scores_glob_array[i, :, 1]), 2)}) - Loss={round(np.mean(scores_glob_array[i, :, 0]), 2)} (+- {round(np.std(scores_glob_array[i, :, 0]), 2)})')\n",
    "\n",
    "idx_best = np.argmax(np.mean(scores_glob_array, 1)[:, 1])\n",
    "history_best = history_list[idx_best]\n",
    "model_best = history_best.model\n",
    "print(f'\\nEl mejor resultado para la DenseNet-CC+MLO se obtiene para LR = {glob_param[idx_best, 0]} y momentum = {glob_param[idx_best, 1]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-madison",
   "metadata": {
    "executionInfo": {
     "elapsed": 2276,
     "status": "ok",
     "timestamp": 1620075964551,
     "user": {
      "displayName": "Duun V",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GipsnLzW9bWFJbKNrxY-F-xi7XKh0HhowJGi0hF9A=s64",
      "userId": "10921869077997462696"
     },
     "user_tz": -120
    },
    "id": "marked-decimal"
   },
   "source": [
    "Visualizamos la evolución de las métricas durante el proceso de entrenamiento de las redes, haciendo la media por época de las distintas iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fatty-fireplace",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 1875046,
     "status": "ok",
     "timestamp": 1621185207599,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "racial-tribute",
    "outputId": "c3df1e7a-60af-4764-aa06-aa2892651588"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(history_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-vitamin",
   "metadata": {
    "id": "t_0z4ogBWHxS"
   },
   "source": [
    "Mostramos la matriz de confusión de la última iteración por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ordinary-indication",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1880397,
     "status": "ok",
     "timestamp": 1621185212960,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "relevant-technique",
    "outputId": "e55ee5bd-0dda-42cc-8e1b-2f18a3270441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            benigno seguimiento  maligno \n",
      "    benigno    25.0         1.0      2.0 \n",
      "seguimiento     2.0         2.0		0.0 \n",
      "    maligno     5.0         1.0     14.0 \n"
     ]
    }
   ],
   "source": [
    "conf_matrix = metrics.confusion_matrix(Y_val.argmax(axis = 1), model_best.predict([X_val_cc, X_val_mlo]).argmax(axis = 1))\n",
    "print_cm(conf_matrix, list(dict_valores.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-indicator",
   "metadata": {
    "id": "viral-constitutional"
   },
   "source": [
    "Mostramos las métricas de resultados según categoría para poder evaluar el desempeño de la red en cada caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "recent-thanksgiving",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1881658,
     "status": "ok",
     "timestamp": 1621185214230,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "gross-aaron",
    "outputId": "4bfecfd9-984a-46b3-a8a9-b468409bdc6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     benigno       0.78      0.89      0.83        28\n",
      " seguimiento       0.50      0.50      0.50         4\n",
      "     maligno       0.88      0.70      0.78        20\n",
      "\n",
      "    accuracy                           0.79        52\n",
      "   macro avg       0.72      0.70      0.70        52\n",
      "weighted avg       0.80      0.79      0.79        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classif_report = metrics.classification_report(Y_val.argmax(axis = 1), model_best.predict([X_val_cc, X_val_mlo]).argmax(axis = 1),\n",
    "                                               target_names = list(dict_valores.keys()))\n",
    "print(classif_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-milwaukee",
   "metadata": {
    "id": "heavy-company"
   },
   "source": [
    "Guardamos el modelo entrenado para su uso en pasos posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baking-delivery",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2028807,
     "status": "ok",
     "timestamp": 1621185361388,
     "user": {
      "displayName": "Iago Veiras Lens",
      "photoUrl": "",
      "userId": "00127513713424041949"
     },
     "user_tz": -120
    },
    "id": "foreign-teens",
    "outputId": "45b2ba83-0a52-4cee-c62b-7e35d3ef4f36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Colab Notebooks/model_CC-MLO/assets\n"
     ]
    }
   ],
   "source": [
    "if google_colab:\n",
    "    file_path = '/content/gdrive/MyDrive/Colab Notebooks/model_best_CC-MLO'\n",
    "else:\n",
    "    file_path = './model_best_CC-MLO'\n",
    "K.models.save_model(model_best, file_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Optimización_DenseNet_2Ramas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
